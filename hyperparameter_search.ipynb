{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from src.utils import prepare_df, linear_prob_combiner\n",
    "from src.incremental_softmax_recovery import incremental_softmax_recovery\n",
    "from src.evaluation import compute_sktr_vs_argmax_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging with selective DEBUG for our modules only\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,           # Set root to INFO (reduces third-party noise)\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    force=True                    # Force override of any existing handlers (useful in Jupyter)\n",
    ")\n",
    "\n",
    "# Enable DEBUG for our specific modules only\n",
    "our_modules = [\n",
    "    'src.classes', \n",
    "    'src.incremental_softmax_recovery', \n",
    "    'src.utils', \n",
    "    'src.conformance_checking',\n",
    "    'src.data_processing',\n",
    "    'src.petri_model',\n",
    "    'src.calibration'\n",
    "]\n",
    "\n",
    "for module_name in our_modules:\n",
    "    logging.getLogger(module_name).setLevel(logging.DEBUG)\n",
    "\n",
    "# Silence noisy third-party libraries\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)  # Only show warnings/errors from graphviz\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)  # Silence matplotlib if present\n",
    "logging.getLogger('PIL').setLevel(logging.WARNING)  # Silence PIL if present\n",
    "\n",
    "print(\"\u2705 Logging configured: DEBUG for our modules, INFO+ for third-party libraries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset\n",
    "dataset_name = '50salads'\n",
    "\n",
    "# Load data\n",
    "result = prepare_df(dataset_name)\n",
    "\n",
    "if len(result) == 2:\n",
    "    df, softmax_lst = result\n",
    "else:\n",
    "    df, softmax_lst, _ = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration and helpers\n",
    "from pathlib import Path\n",
    "\n",
    "alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "interpolation_strategies = {\n",
    "    'unigram_super_heavy': [0.75, 0.15, 0.1],\n",
    "    'unigram_heavy': [0.6, 0.25, 0.15],\n",
    "    'balanced': [0.4, 0.35, 0.25],\n",
    "    'trigram_heavy': [0.15, 0.25, 0.6],\n",
    "}\n",
    "\n",
    "TRAIN_TRACE_SWEEP = {\n",
    "    '50salads': list(range(1, 11)),\n",
    "    'gtea': list(range(1, 8)),\n",
    "}\n",
    "PARALLELIZATION_STRATEGY = 'dataset'  # 'trace', 'dataset', 'both', or None\n",
    "MAX_WORKERS = None\n",
    "HYPERPARAM_N_JOBS = 40\n",
    "SAVE_PROCESS_MODEL_ONCE = True\n",
    "\n",
    "\n",
    "def get_notebook_directory() -> Path:\n",
    "    '''Best effort to locate the directory that contains this notebook.'''\n",
    "    try:\n",
    "        import ipynbname\n",
    "        return Path(ipynbname.path()).parent.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ipython = get_ipython()\n",
    "        if ipython:\n",
    "            return Path(ipython.run_line_magic('pwd', '')).resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "def resolve_results_dir(dataset: str) -> Path:\n",
    "    nb_dir = get_notebook_directory()\n",
    "    results_dir = nb_dir / 'results' / dataset\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\ud83d\udcc1 Notebook directory: {nb_dir}\")\n",
    "    print(f\"\ud83d\udcc1 Results directory: {results_dir}\")\n",
    "    return results_dir\n",
    "\n",
    "\n",
    "def build_base_config(dataset: str, results_dir: Path) -> dict:\n",
    "    base = {\n",
    "        'n_train_traces': 7,\n",
    "        'n_test_traces': None,\n",
    "        'train_cases': None,\n",
    "        'test_cases': None,\n",
    "        'ensure_train_variant_diversity': True,\n",
    "        'ensure_test_variant_diversity': True,\n",
    "        'use_same_traces_for_train_test': False,\n",
    "        'allow_train_cases_in_test': True,\n",
    "        'compute_marking_transition_map': True,\n",
    "        'sequential_sampling': True,\n",
    "        'n_indices': None,\n",
    "        'n_per_run': 10000,\n",
    "        'independent_sampling': True,\n",
    "        'prob_threshold': 1e-6,\n",
    "        'chunk_size': 11,\n",
    "        'conformance_switch_penalty_weight': 1.0,\n",
    "        'merge_mismatched_boundaries': False,\n",
    "        'conditioning_combine_fn': linear_prob_combiner,\n",
    "        'max_hist_len': 3,\n",
    "        'conditioning_n_prev_labels': 3,\n",
    "        'use_collapsed_runs': True,\n",
    "        'cost_function': \"linear\",\n",
    "        'model_move_cost': 1.0,\n",
    "        'log_move_cost': 1.0,\n",
    "        'tau_move_cost': 0.0,\n",
    "        'non_sync_penalty': 1.0,\n",
    "        'use_calibration': True,\n",
    "        'temp_bounds': (1.0, 10.0),\n",
    "        'temperature': None,\n",
    "        'verbose': True,\n",
    "        'log_level': logging.INFO,\n",
    "        'round_precision': 2,\n",
    "        'random_seed': 101,\n",
    "        'save_model_path': str(results_dir / f'discovered_petri_net_{dataset}'),\n",
    "        'save_model': True,\n",
    "        'dataset_name': dataset,\n",
    "        'parallel_processing': False,\n",
    "        'dataset_parallelization': False,\n",
    "        'max_workers': MAX_WORKERS,\n",
    "    }\n",
    "    parallel_modes = {\n",
    "        'trace': (True, False),\n",
    "        'dataset': (False, True),\n",
    "        'both': (False, True),\n",
    "        None: (False, False),\n",
    "    }\n",
    "    base['parallel_processing'], base['dataset_parallelization'] = parallel_modes.get(\n",
    "        PARALLELIZATION_STRATEGY, (False, False)\n",
    "    )\n",
    "    print(\n",
    "        f\"\ud83d\udd27 Parallelization: {PARALLELIZATION_STRATEGY or 'none'} \"\n",
    "        f\"(max_workers={MAX_WORKERS or 'auto'})\"\n",
    "    )\n",
    "    return base\n",
    "\n",
    "\n",
    "results_dir = resolve_results_dir(dataset_name)\n",
    "base_config = build_base_config(dataset_name, results_dir)\n",
    "\n",
    "train_trace_values = TRAIN_TRACE_SWEEP.get(dataset_name.lower())\n",
    "if not train_trace_values:\n",
    "    train_trace_values = [base_config['n_train_traces']]\n",
    "    print(\n",
    "        f\"\u26a0\ufe0f Dataset '{dataset_name}' not in TRAIN_TRACE_SWEEP; \"\n",
    "        f\"using n_train_traces={train_trace_values[-1]}.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\ud83e\uddea Training-trace sweep: {train_trace_values}\")\n",
    "base_config['n_train_traces'] = train_trace_values[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter search\n",
    "\n",
    "def run_single_hyperparameter(\n",
    "    n_train_traces,\n",
    "    alpha,\n",
    "    strategy_name,\n",
    "    weights,\n",
    "    idx,\n",
    "    total_runs,\n",
    "    df,\n",
    "    softmax_lst,\n",
    "    base_config,\n",
    "    results_dir,\n",
    "):\n",
    "    dataset_name = base_config.get('dataset_name', 'unknown_dataset')\n",
    "    print(\n",
    "        f\"[{idx}/{total_runs}] Running on {dataset_name}: \"\n",
    "        f\"train_traces={n_train_traces}, alpha={alpha}, strategy={strategy_name}\"\n",
    "    )\n",
    "\n",
    "    run_config = base_config.copy()\n",
    "    run_config['conditioning_alpha'] = alpha\n",
    "    run_config['conditioning_interpolation_weights'] = weights\n",
    "    run_config['n_train_traces'] = n_train_traces\n",
    "\n",
    "    save_model_this_run = run_config.get('save_model', False)\n",
    "    if SAVE_PROCESS_MODEL_ONCE:\n",
    "        save_model_this_run = (idx == 1)\n",
    "    run_config['save_model'] = save_model_this_run\n",
    "    run_config['save_model_path'] = base_config['save_model_path']\n",
    "\n",
    "    run_config.pop('dataset_name', None)\n",
    "\n",
    "    results_df, accuracy_dict, prob_dict_uncollapsed, prob_dict_collapsed = incremental_softmax_recovery(\n",
    "        df=df,\n",
    "        softmax_lst=softmax_lst,\n",
    "        **run_config,\n",
    "    )\n",
    "\n",
    "    csv_filename = (\n",
    "        f\"{dataset_name}_train_{n_train_traces}_hyperparam_search_alpha_{alpha}_\"\n",
    "        f\"weights_{strategy_name}.csv\"\n",
    "    )\n",
    "    csv_path = os.path.join(results_dir, csv_filename)\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    metrics = compute_sktr_vs_argmax_metrics(\n",
    "        csv_path,\n",
    "        case_col='case:concept:name',\n",
    "        sktr_pred_col='sktr_activity',\n",
    "        argmax_pred_col='argmax_activity',\n",
    "        gt_col='ground_truth',\n",
    "        background=0,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'dataset_name': dataset_name,\n",
    "        'n_train_traces': n_train_traces,\n",
    "        'conditioning_alpha': alpha,\n",
    "        'interpolation_weights_strategy': strategy_name,\n",
    "        'interpolation_weights': str(weights),\n",
    "        'sktr_acc_micro': metrics['sktr']['acc_micro'],\n",
    "        'sktr_edit': metrics['sktr']['edit'],\n",
    "        'sktr_f1@10': metrics['sktr']['f1@10'],\n",
    "        'sktr_f1@25': metrics['sktr']['f1@25'],\n",
    "        'sktr_f1@50': metrics['sktr']['f1@50'],\n",
    "        'argmax_acc_micro': metrics['argmax']['acc_micro'],\n",
    "        'argmax_edit': metrics['argmax']['edit'],\n",
    "        'argmax_f1@10': metrics['argmax']['f1@10'],\n",
    "        'argmax_f1@25': metrics['argmax']['f1@25'],\n",
    "        'argmax_f1@50': metrics['argmax']['f1@50'],\n",
    "        'results_csv_path': csv_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# Build parameter grid and run experiments\n",
    "param_combinations = list(product(train_trace_values, alphas, interpolation_strategies.items()))\n",
    "total_runs = len(param_combinations)\n",
    "\n",
    "print(\n",
    "    f\"Starting hyperparameter grid: {len(train_trace_values)} train-trace settings \u00d7 \"\n",
    "    f\"{len(alphas)} alphas \u00d7 {len(interpolation_strategies)} strategies = {total_runs} runs\"\n",
    ")\n",
    "print(f\"Hyperparameter-level parallelization: joblib (n_jobs={HYPERPARAM_N_JOBS})\")\n",
    "print(f\"Run-level parallelization: {PARALLELIZATION_STRATEGY or 'none'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_rows = Parallel(n_jobs=HYPERPARAM_N_JOBS, verbose=10)(\n",
    "    delayed(run_single_hyperparameter)(\n",
    "        n_train_traces,\n",
    "        alpha,\n",
    "        strategy_name,\n",
    "        weights,\n",
    "        idx,\n",
    "        total_runs,\n",
    "        df,\n",
    "        softmax_lst,\n",
    "        base_config,\n",
    "        results_dir,\n",
    "    )\n",
    "    for idx, (n_train_traces, alpha, (strategy_name, weights)) in enumerate(param_combinations, 1)\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Display and save results\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER SEARCH SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Top 10 combinations by SKTR Accuracy:\")\n",
    "display(summary_df.sort_values('sktr_acc_micro', ascending=False).head(10))\n",
    "print()\n",
    "print(\"Full results (sorted by SKTR Accuracy):\")\n",
    "display(summary_df.sort_values('sktr_acc_micro', ascending=False))\n",
    "\n",
    "summary_path = os.path.join(\n",
    "    results_dir,\n",
    "    f\"{base_config['dataset_name']}_hyperparameter_search_summary.csv\",\n",
    ")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print()\n",
    "print(f\"\u2713 Summary saved to: {summary_path}\")\n",
    "print(f\"\u2713 Total runs completed: {len(summary_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}