{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import os\n",
        "from itertools import product\n",
        "from src.utils import prepare_df, linear_prob_combiner\n",
        "from src.incremental_softmax_recovery import incremental_softmax_recovery\n",
        "from src.evaluation import compute_sktr_vs_argmax_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging with selective DEBUG for our modules only\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,           # Set root to INFO (reduces third-party noise)\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    force=True                    # Force override of any existing handlers (useful in Jupyter)\n",
        ")\n",
        "\n",
        "# Enable DEBUG for our specific modules only\n",
        "our_modules = [\n",
        "    'src.classes', \n",
        "    'src.incremental_softmax_recovery', \n",
        "    'src.utils', \n",
        "    'src.conformance_checking',\n",
        "    'src.data_processing',\n",
        "    'src.petri_model',\n",
        "    'src.calibration'\n",
        "]\n",
        "\n",
        "for module_name in our_modules:\n",
        "    logging.getLogger(module_name).setLevel(logging.DEBUG)\n",
        "\n",
        "# Silence noisy third-party libraries\n",
        "logging.getLogger('graphviz').setLevel(logging.WARNING)  # Only show warnings/errors from graphviz\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)  # Silence matplotlib if present\n",
        "logging.getLogger('PIL').setLevel(logging.WARNING)  # Silence PIL if present\n",
        "\n",
        "print(\"✅ Logging configured: DEBUG for our modules, INFO+ for third-party libraries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "result = prepare_df('50salads')\n",
        "\n",
        "if len(result) == 2:\n",
        "    df, softmax_lst = result\n",
        "else:\n",
        "    df, softmax_lst, _ = result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter Grid Search for Conditioning Parameters\n",
        "# Define hyperparameter grid\n",
        "alphas = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "interpolation_strategies = {\n",
        "    'unigram_heavy': [0.6, 0.25, 0.15],\n",
        "    'balanced': [0.4, 0.35, 0.25],\n",
        "    'trigram_heavy': [0.15, 0.25, 0.6]\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "summary_rows = []\n",
        "results_dir = 'results'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Base config\n",
        "base_config = {\n",
        "    # === Data Splitting ===\n",
        "    'n_train_traces': 10,\n",
        "    'n_test_traces': 10,\n",
        "    'train_cases': None,\n",
        "    'test_cases': None,\n",
        "    'ensure_train_variant_diversity': True,\n",
        "    'ensure_test_variant_diversity': True,\n",
        "    'use_same_traces_for_train_test': False,\n",
        "    'compute_marking_transition_map': True,\n",
        "\n",
        "    # === Sampling Configuration ===\n",
        "    'sequential_sampling': True,\n",
        "    'n_indices': None,\n",
        "    'n_per_run': 10000,\n",
        "    'independent_sampling': True,\n",
        "\n",
        "    # === Conformance Parameters ===\n",
        "    'prob_threshold': 1e-6,\n",
        "    'chunk_size': 11,\n",
        "    'conformance_switch_penalty_weight': 1.0,\n",
        "    'merge_mismatched_boundaries': False,\n",
        "\n",
        "    # === Conditioned Probability Adjustment ===\n",
        "    'conditioning_combine_fn': linear_prob_combiner,\n",
        "    \n",
        "    # === Multi-Label Conditioning ===\n",
        "    'max_hist_len': 3,\n",
        "    'conditioning_n_prev_labels': 3,\n",
        "    'use_collapsed_runs': True,\n",
        "    \n",
        "    # === Cost Function ===\n",
        "    'cost_function': \"linear\",\n",
        "    'model_move_cost': 1.0,\n",
        "    'log_move_cost': 1.0,\n",
        "    'tau_move_cost': 0.0,\n",
        "    'non_sync_penalty': 1.0,\n",
        "\n",
        "    # === Temperature Calibration ===\n",
        "    'use_calibration': True,\n",
        "    'temp_bounds': (1.0, 10.0),\n",
        "    'temperature': None,\n",
        "\n",
        "    # === Logging ===\n",
        "    'verbose': True,\n",
        "    'log_level': logging.INFO,\n",
        "\n",
        "    # === Miscellaneous ===\n",
        "    'round_precision': 2,\n",
        "    'random_seed': 101,\n",
        "    'save_model_path': \"./results/discovered_petri_net\",\n",
        "    'save_model': True,\n",
        "}\n",
        "\n",
        "print(f\"Starting hyperparameter grid search: {len(alphas)} alphas × {len(interpolation_strategies)} strategies = {len(alphas) * len(interpolation_strategies)} total runs\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Grid search loop\n",
        "for idx, (alpha, (strategy_name, weights)) in enumerate(product(alphas, interpolation_strategies.items()), 1):\n",
        "    print(f\"\\n[{idx}/{len(alphas) * len(interpolation_strategies)}] Running: alpha={alpha}, strategy={strategy_name}, weights={weights}\")\n",
        "    \n",
        "    # Update config with current hyperparameters\n",
        "    run_config = base_config.copy()\n",
        "    run_config['conditioning_alpha'] = alpha\n",
        "    run_config['conditioning_interpolation_weights'] = weights\n",
        "    \n",
        "    # Run recovery\n",
        "    output = incremental_softmax_recovery(\n",
        "        df=df,\n",
        "        softmax_lst=softmax_lst,\n",
        "        **run_config \n",
        "    )\n",
        "    \n",
        "    # Unpack results\n",
        "    results_df, accuracy_dict, prob_dict = output\n",
        "    \n",
        "    # Save results CSV\n",
        "    csv_filename = f'hyperparam_search_alpha_{alpha}_weights_{strategy_name}.csv'\n",
        "    csv_path = os.path.join(results_dir, csv_filename)\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "    print(f\"  ✓ Saved results to: {csv_path}\")\n",
        "    \n",
        "    # Compute metrics\n",
        "    metrics = compute_sktr_vs_argmax_metrics(\n",
        "        csv_path,\n",
        "        case_col='case:concept:name',\n",
        "        sktr_pred_col='sktr_activity',\n",
        "        argmax_pred_col='argmax_activity',\n",
        "        gt_col='ground_truth',\n",
        "        background=0\n",
        "    )\n",
        "    \n",
        "    # Store summary row\n",
        "    summary_rows.append({\n",
        "        'conditioning_alpha': alpha,\n",
        "        'interpolation_weights_strategy': strategy_name,\n",
        "        'interpolation_weights': str(weights),  # Convert to string for CSV compatibility\n",
        "        'sktr_acc_micro': metrics['sktr']['acc_micro'],\n",
        "        'sktr_edit': metrics['sktr']['edit'],\n",
        "        'sktr_f1@10': metrics['sktr']['f1@10'],\n",
        "        'sktr_f1@25': metrics['sktr']['f1@25'],\n",
        "        'sktr_f1@50': metrics['sktr']['f1@50'],\n",
        "        'argmax_acc_micro': metrics['argmax']['acc_micro'],\n",
        "        'argmax_edit': metrics['argmax']['edit'],\n",
        "        'argmax_f1@10': metrics['argmax']['f1@10'],\n",
        "        'argmax_f1@25': metrics['argmax']['f1@25'],\n",
        "        'argmax_f1@50': metrics['argmax']['f1@50'],\n",
        "        'results_csv_path': csv_path\n",
        "    })\n",
        "    \n",
        "    print(f\"  ✓ SKTR metrics: edit={metrics['sktr']['edit']:.2f}, f1@10={metrics['sktr']['f1@10']:.2f}\")\n",
        "\n",
        "# Create summary dataframe\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "# Display sorted by best sktr_acc_micro (descending - higher is better)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"HYPERPARAMETER SEARCH SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTop 10 combinations by SKTR Accuracy (higher is better):\")\n",
        "display(summary_df.sort_values('sktr_acc_micro', ascending=False).head(10))\n",
        "\n",
        "print(\"\\nFull Results Summary (sorted by SKTR Accuracy, descending):\")\n",
        "display(summary_df.sort_values('sktr_acc_micro', ascending=False))\n",
        "\n",
        "# Save summary\n",
        "summary_path = os.path.join(results_dir, 'hyperparameter_search_summary.csv')\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"\\n✓ Summary saved to: {summary_path}\")\n",
        "print(f\"✓ Total runs completed: {len(summary_rows)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
