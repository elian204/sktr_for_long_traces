{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import os\n",
        "from itertools import product\n",
        "from joblib import Parallel, delayed\n",
        "from src.utils import prepare_df, linear_prob_combiner\n",
        "from src.incremental_softmax_recovery import incremental_softmax_recovery\n",
        "from src.evaluation import compute_sktr_vs_argmax_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging with selective DEBUG for our modules only\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,           # Set root to INFO (reduces third-party noise)\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    force=True                    # Force override of any existing handlers (useful in Jupyter)\n",
        ")\n",
        "\n",
        "# Enable DEBUG for our specific modules only\n",
        "our_modules = [\n",
        "    'src.classes', \n",
        "    'src.incremental_softmax_recovery', \n",
        "    'src.utils', \n",
        "    'src.conformance_checking',\n",
        "    'src.data_processing',\n",
        "    'src.petri_model',\n",
        "    'src.calibration'\n",
        "]\n",
        "\n",
        "for module_name in our_modules:\n",
        "    logging.getLogger(module_name).setLevel(logging.DEBUG)\n",
        "\n",
        "# Silence noisy third-party libraries\n",
        "logging.getLogger('graphviz').setLevel(logging.WARNING)  # Only show warnings/errors from graphviz\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)  # Silence matplotlib if present\n",
        "logging.getLogger('PIL').setLevel(logging.WARNING)  # Silence PIL if present\n",
        "\n",
        "print(\"‚úÖ Logging configured: DEBUG for our modules, INFO+ for third-party libraries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "result = prepare_df('50salads')\n",
        "\n",
        "if len(result) == 2:\n",
        "    df, softmax_lst = result\n",
        "else:\n",
        "    df, softmax_lst, _ = result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter Grid Search for Conditioning Parameters (Parallelized)\n",
        "# Define hyperparameter grid\n",
        "alphas = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "interpolation_strategies = {\n",
        "    'unigram_heavy': [0.6, 0.25, 0.15],\n",
        "    'balanced': [0.4, 0.35, 0.25],\n",
        "    'trigram_heavy': [0.15, 0.25, 0.6]\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# PARALLELIZATION CONFIGURATION\n",
        "# ============================================================================\n",
        "# Choose parallelization strategy for each hyperparameter run:\n",
        "#   - None: Sequential processing (no parallelization within each run)\n",
        "#   - 'trace': Trace-level parallelization (parallel_processing=True)\n",
        "#              Each test trace processed independently in parallel\n",
        "#   - 'dataset': Dataset-level parallelization (dataset_parallelization=True)\n",
        "#                Test dataset split into chunks, processed in parallel\n",
        "#                Model discovered once and shared across workers\n",
        "#   - 'both': Use both hyperparameter-level (joblib) AND dataset-level parallelization\n",
        "#             (hyperparameter runs still parallelized by joblib, but each run also\n",
        "#              uses dataset-level parallelization internally)\n",
        "#\n",
        "# Recommendation:\n",
        "#   - 'dataset': Best for large test sets where each trace is relatively fast\n",
        "#   - 'trace': Best when traces are long/complex and benefit from independent processing\n",
        "#   - None: Best for debugging or when overhead outweighs benefits\n",
        "# ============================================================================\n",
        "PARALLELIZATION_STRATEGY = 'dataset'  # Options: None, 'trace', 'dataset', 'both'\n",
        "MAX_WORKERS = None  # None = auto-detect (use all CPUs), or specify number like 4\n",
        "\n",
        "print(f\"üîß Parallelization strategy: {PARALLELIZATION_STRATEGY}\")\n",
        "print(f\"üîß Max workers: {MAX_WORKERS if MAX_WORKERS else 'auto (all CPUs)'}\")\n",
        "\n",
        "# Results storage - ensure it's created next to the notebook file\n",
        "# Get notebook's directory (works in both local and server Jupyter environments)\n",
        "# This ensures results are always saved next to the notebook file, regardless of working directory\n",
        "\n",
        "def get_notebook_directory():\n",
        "    \"\"\"\n",
        "    Get the directory containing the current notebook file.\n",
        "    Tries multiple methods to ensure it works in different Jupyter environments.\n",
        "    \"\"\"\n",
        "    # Method 1: Try using ipynbname library (most reliable, but requires installation)\n",
        "    try:\n",
        "        import ipynbname\n",
        "        nb_path = ipynbname.path()\n",
        "        return os.path.dirname(os.path.abspath(nb_path))\n",
        "    except ImportError:\n",
        "        pass  # Library not installed, try next method\n",
        "    except Exception:\n",
        "        pass  # Other error, try next method\n",
        "    \n",
        "    # Method 2: Use IPython's %pwd magic (works in most Jupyter setups)\n",
        "    # In standard Jupyter/Lab, the working directory is set to the notebook's directory\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        ipython = get_ipython()\n",
        "        if ipython:\n",
        "            notebook_dir = ipython.run_line_magic('pwd', '')\n",
        "            # Verify this directory contains .ipynb files (sanity check)\n",
        "            import glob\n",
        "            if glob.glob(os.path.join(notebook_dir, '*.ipynb')):\n",
        "                return notebook_dir\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    # Method 3: Fallback to current working directory\n",
        "    # This works if Jupyter sets cwd to notebook directory (which it usually does)\n",
        "    return os.getcwd()\n",
        "\n",
        "# Get notebook directory using the function above\n",
        "notebook_dir = get_notebook_directory()\n",
        "notebook_dir = os.path.abspath(notebook_dir)  # Ensure absolute path\n",
        "\n",
        "# Create results folder in the same directory as the notebook\n",
        "# This ensures results are ALWAYS saved next to the notebook file, regardless of where code is run\n",
        "results_dir = os.path.join(notebook_dir, 'results')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "results_abs_path = os.path.abspath(results_dir)\n",
        "\n",
        "# Verify the setup\n",
        "print(f\"üìÅ Notebook directory: {notebook_dir}\")\n",
        "print(f\"üìÅ Results folder location: {results_abs_path}\")\n",
        "print(f\"üìÅ Results folder exists: {os.path.exists(results_dir)}\")\n",
        "print(f\"‚úÖ Results will ALWAYS be saved in: {results_abs_path}\")\n",
        "print(f\"   (This folder is next to the notebook file, regardless of where you run the code)\")\n",
        "print(f\"üí° For maximum reliability, install: pip install ipynbname\")\n",
        "\n",
        "# Base config\n",
        "base_config = {\n",
        "    # === Data Splitting ===\n",
        "    'n_train_traces': 10,\n",
        "    'n_test_traces': None,  # None = use all available cases (including training cases if allow_train_cases_in_test=True)\n",
        "    'train_cases': None,\n",
        "    'test_cases': None,\n",
        "    'ensure_train_variant_diversity': True,\n",
        "    'ensure_test_variant_diversity': True,\n",
        "    'use_same_traces_for_train_test': False,\n",
        "    'allow_train_cases_in_test': True,  # Set to True to include training cases in test set\n",
        "    'compute_marking_transition_map': True,\n",
        "\n",
        "    # === Sampling Configuration ===\n",
        "    'sequential_sampling': True,\n",
        "    'n_indices': None,\n",
        "    'n_per_run': 10000,\n",
        "    'independent_sampling': True,\n",
        "\n",
        "    # === Conformance Parameters ===\n",
        "    'prob_threshold': 1e-6,\n",
        "    'chunk_size': 11,\n",
        "    'conformance_switch_penalty_weight': 1.0,\n",
        "    'merge_mismatched_boundaries': False,\n",
        "\n",
        "    # === Conditioned Probability Adjustment ===\n",
        "    'conditioning_combine_fn': linear_prob_combiner,\n",
        "    \n",
        "    # === Multi-Label Conditioning ===\n",
        "    'max_hist_len': 3,\n",
        "    'conditioning_n_prev_labels': 3,\n",
        "    'use_collapsed_runs': True,\n",
        "    \n",
        "    # === Cost Function ===\n",
        "    'cost_function': \"linear\",\n",
        "    'model_move_cost': 1.0,\n",
        "    'log_move_cost': 1.0,\n",
        "    'tau_move_cost': 0.0,\n",
        "    'non_sync_penalty': 1.0,\n",
        "\n",
        "    # === Temperature Calibration ===\n",
        "    'use_calibration': True,\n",
        "    'temp_bounds': (1.0, 10.0),\n",
        "    'temperature': None,\n",
        "\n",
        "    # === Logging ===\n",
        "    'verbose': True,\n",
        "    'log_level': logging.INFO,\n",
        "\n",
        "    # === Miscellaneous ===\n",
        "    'round_precision': 2,\n",
        "    'random_seed': 101,\n",
        "    'save_model_path': None,  # Will be set below after results_dir is defined\n",
        "    'save_model': True,\n",
        "    \n",
        "    # === Parallelization (set based on PARALLELIZATION_STRATEGY) ===\n",
        "    'parallel_processing': False,\n",
        "    'dataset_parallelization': False,\n",
        "    'max_workers': MAX_WORKERS,\n",
        "}\n",
        "\n",
        "# Update base_config with absolute path for save_model_path\n",
        "base_config['save_model_path'] = os.path.join(results_dir, 'discovered_petri_net')\n",
        "\n",
        "# Set parallelization flags based on strategy\n",
        "if PARALLELIZATION_STRATEGY == 'trace':\n",
        "    base_config['parallel_processing'] = True\n",
        "    base_config['dataset_parallelization'] = False\n",
        "elif PARALLELIZATION_STRATEGY == 'dataset':\n",
        "    base_config['parallel_processing'] = False\n",
        "    base_config['dataset_parallelization'] = True\n",
        "elif PARALLELIZATION_STRATEGY == 'both':\n",
        "    base_config['parallel_processing'] = False\n",
        "    base_config['dataset_parallelization'] = True  # Use dataset-level for internal parallelization\n",
        "elif PARALLELIZATION_STRATEGY is None:\n",
        "    base_config['parallel_processing'] = False\n",
        "    base_config['dataset_parallelization'] = False\n",
        "else:\n",
        "    raise ValueError(f\"Invalid PARALLELIZATION_STRATEGY: {PARALLELIZATION_STRATEGY}. Must be None, 'trace', 'dataset', or 'both'\")\n",
        "\n",
        "print(f\"‚úì Configured parallelization: parallel_processing={base_config['parallel_processing']}, dataset_parallelization={base_config['dataset_parallelization']}\")\n",
        "\n",
        "def run_single_hyperparameter(alpha, strategy_name, weights, idx, total_runs, df, softmax_lst, base_config, results_dir):\n",
        "    \"\"\"Run a single hyperparameter combination\"\"\"\n",
        "    print(f\"[{idx}/{total_runs}] Running: alpha={alpha}, strategy={strategy_name}, weights={weights}\")\n",
        "    \n",
        "    # Update config with current hyperparameters\n",
        "    run_config = base_config.copy()\n",
        "    run_config['conditioning_alpha'] = alpha\n",
        "    run_config['conditioning_interpolation_weights'] = weights\n",
        "    # Use unique save path per run to avoid overwrites (use absolute path)\n",
        "    run_config['save_model_path'] = os.path.join(results_dir, f'discovered_petri_net_alpha_{alpha}_weights_{strategy_name}')\n",
        "    \n",
        "    # Run recovery\n",
        "    output = incremental_softmax_recovery(\n",
        "        df=df,\n",
        "        softmax_lst=softmax_lst,\n",
        "        **run_config \n",
        "    )\n",
        "    \n",
        "    # Unpack results\n",
        "    results_df, accuracy_dict, prob_dict = output\n",
        "    \n",
        "    # Save results CSV\n",
        "    csv_filename = f'hyperparam_search_alpha_{alpha}_weights_{strategy_name}.csv'\n",
        "    csv_path = os.path.join(results_dir, csv_filename)\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "    print(f\"  [{idx}/{total_runs}] ‚úì Saved results to: {csv_path}\")\n",
        "    \n",
        "    # Compute metrics\n",
        "    metrics = compute_sktr_vs_argmax_metrics(\n",
        "        csv_path,\n",
        "        case_col='case:concept:name',\n",
        "        sktr_pred_col='sktr_activity',\n",
        "        argmax_pred_col='argmax_activity',\n",
        "        gt_col='ground_truth',\n",
        "        background=0\n",
        "    )\n",
        "    \n",
        "    # Return summary row\n",
        "    result = {\n",
        "        'conditioning_alpha': alpha,\n",
        "        'interpolation_weights_strategy': strategy_name,\n",
        "        'interpolation_weights': str(weights),  # Convert to string for CSV compatibility\n",
        "        'sktr_acc_micro': metrics['sktr']['acc_micro'],\n",
        "        'sktr_edit': metrics['sktr']['edit'],\n",
        "        'sktr_f1@10': metrics['sktr']['f1@10'],\n",
        "        'sktr_f1@25': metrics['sktr']['f1@25'],\n",
        "        'sktr_f1@50': metrics['sktr']['f1@50'],\n",
        "        'argmax_acc_micro': metrics['argmax']['acc_micro'],\n",
        "        'argmax_edit': metrics['argmax']['edit'],\n",
        "        'argmax_f1@10': metrics['argmax']['f1@10'],\n",
        "        'argmax_f1@25': metrics['argmax']['f1@25'],\n",
        "        'argmax_f1@50': metrics['argmax']['f1@50'],\n",
        "        'results_csv_path': csv_path\n",
        "    }\n",
        "    \n",
        "    print(f\"  [{idx}/{total_runs}] ‚úì SKTR metrics: edit={metrics['sktr']['edit']:.2f}, f1@10={metrics['sktr']['f1@10']:.2f}\")\n",
        "    return result\n",
        "\n",
        "# Prepare all parameter combinations\n",
        "param_combinations = list(product(alphas, interpolation_strategies.items()))\n",
        "total_runs = len(param_combinations)\n",
        "\n",
        "print(f\"Starting hyperparameter grid search: {len(alphas)} alphas √ó {len(interpolation_strategies)} strategies = {total_runs} total runs\")\n",
        "print(f\"Hyperparameter-level parallelization: joblib (across hyperparameter combinations)\")\n",
        "if PARALLELIZATION_STRATEGY:\n",
        "    print(f\"Run-level parallelization: {PARALLELIZATION_STRATEGY} (within each hyperparameter run)\")\n",
        "else:\n",
        "    print(f\"Run-level parallelization: None (sequential)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run hyperparameter combinations in parallel using joblib\n",
        "# Note: If PARALLELIZATION_STRATEGY == 'both', we get two levels of parallelization:\n",
        "#   - Outer: joblib parallelizes across hyperparameter combinations\n",
        "#   - Inner: dataset_parallelization splits test dataset within each run\n",
        "n_jobs = -1  # Use all available CPUs for hyperparameter-level parallelization\n",
        "summary_rows = Parallel(n_jobs=n_jobs, verbose=10)(\n",
        "    delayed(run_single_hyperparameter)(\n",
        "        alpha, strategy_name, weights, idx, total_runs, df, softmax_lst, base_config, results_dir\n",
        "    )\n",
        "    for idx, (alpha, (strategy_name, weights)) in enumerate(param_combinations, 1)\n",
        ")\n",
        "\n",
        "# Create summary dataframe\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "# Display sorted by best sktr_acc_micro (descending - higher is better)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"HYPERPARAMETER SEARCH SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTop 10 combinations by SKTR Accuracy (higher is better):\")\n",
        "display(summary_df.sort_values('sktr_acc_micro', ascending=False).head(10))\n",
        "\n",
        "print(\"\\nFull Results Summary (sorted by SKTR Accuracy, descending):\")\n",
        "display(summary_df.sort_values('sktr_acc_micro', ascending=False))\n",
        "\n",
        "# Save summary\n",
        "summary_path = os.path.join(results_dir, 'hyperparameter_search_summary.csv')\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"\\n‚úì Summary saved to: {summary_path}\")\n",
        "print(f\"‚úì Total runs completed: {len(summary_rows)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "researchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
