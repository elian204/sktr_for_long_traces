{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workspace root added to sys.path: C:\\Users\\User\\Jupyter Projects\\sktr_for_long_traces-1\n",
      "‚úÖ Found src directory at: C:\\Users\\User\\Jupyter Projects\\sktr_for_long_traces-1\\src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def detect_workspace_root() -> Path:\n",
    "    \"\"\"Locate project root by finding a nearby src directory; avoid hardcoded paths.\"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    # Notebook path (if available and valid)\n",
    "    try:\n",
    "        import ipynbname\n",
    "        nb_path = Path(ipynbname.path()).resolve()\n",
    "        if nb_path.exists():\n",
    "            candidates.append(nb_path.parent)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Current working directory and its parent\n",
    "    candidates.append(Path.cwd())\n",
    "    candidates.append(Path.cwd().parent)\n",
    "\n",
    "    for candidate in candidates:\n",
    "        if (candidate / 'src').exists():\n",
    "            return candidate\n",
    "\n",
    "    # Fallback to the first candidate if none contain src\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "workspace_root = detect_workspace_root()\n",
    "workspace_root_str = str(workspace_root)\n",
    "\n",
    "# Ensure the workspace root directory is in sys.path (so src can be imported)\n",
    "if workspace_root_str not in sys.path:\n",
    "    sys.path.insert(0, workspace_root_str)\n",
    "elif sys.path.index(workspace_root_str) != 0:\n",
    "    # Move to front if it exists but isn't first\n",
    "    sys.path.remove(workspace_root_str)\n",
    "    sys.path.insert(0, workspace_root_str)\n",
    "\n",
    "# Diagnostic: Verify src can be found\n",
    "src_path = workspace_root / 'src'\n",
    "if src_path.exists():\n",
    "    print(f\"‚úÖ Workspace root added to sys.path: {workspace_root}\")\n",
    "    print(f\"‚úÖ Found src directory at: {src_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: src directory not found at {src_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå The Jupyter kernel is NOT running in tmux.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from src.utils import prepare_df, linear_prob_combiner\n",
    "from src.incremental_softmax_recovery import incremental_softmax_recovery\n",
    "from src.evaluation import compute_sktr_vs_argmax_metrics\n",
    "\n",
    "if \"TMUX\" in os.environ:\n",
    "    print(\"‚úÖ The Jupyter kernel is running INSIDE a tmux session.\")\n",
    "    print(\"TMUX variable:\", os.environ[\"TMUX\"])\n",
    "else:\n",
    "    print(\"‚ùå The Jupyter kernel is NOT running in tmux.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logging configured: DEBUG for our modules, INFO+ for third-party libraries\n"
     ]
    }
   ],
   "source": [
    "# Configure logging with selective DEBUG for our modules only\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,           # Set root to INFO (reduces third-party noise)\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    force=True                    # Force override of any existing handlers (useful in Jupyter)\n",
    ")\n",
    "\n",
    "# Enable DEBUG for our specific modules only\n",
    "our_modules = [\n",
    "    'src.classes', \n",
    "    'src.incremental_softmax_recovery', \n",
    "    'src.utils', \n",
    "    'src.conformance_checking',\n",
    "    'src.data_processing',\n",
    "    'src.petri_model',\n",
    "    'src.calibration'\n",
    "]\n",
    "\n",
    "for module_name in our_modules:\n",
    "    logging.getLogger(module_name).setLevel(logging.DEBUG)\n",
    "\n",
    "# Silence noisy third-party libraries\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)  # Only show warnings/errors from graphviz\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)  # Silence matplotlib if present\n",
    "logging.getLogger('PIL').setLevel(logging.WARNING)  # Silence PIL if present\n",
    "\n",
    "print(\"‚úÖ Logging configured: DEBUG for our modules, INFO+ for third-party libraries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset\n",
    "dataset_name = '50salads'\n",
    "\n",
    "# Load data\n",
    "result = prepare_df(dataset_name)\n",
    "\n",
    "if len(result) == 2:\n",
    "    df, softmax_lst = result\n",
    "else:\n",
    "    df, softmax_lst, _ = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment configuration and helpers\nfrom pathlib import Path\n\nalphas = [0.05]\ninterpolation_strategies = {\n    'unigram_super_heavy': [0.75, 0.15, 0.1]\n}\n\nTRAIN_TRACE_SWEEP = {\n    '50salads': list(range(1, 11)),\n    'gtea': list(range(1, 8)),\n}\nPARALLELIZATION_STRATEGY = 'trace'  # 'trace', 'dataset', 'both', or None\nMAX_WORKERS = 10  # trace-level workers\nHYPERPARAM_N_JOBS = 1  # avoid nested multiprocessing; inner trace-level handles parallelism\nSAVE_PROCESS_MODEL_ONCE = True\n\n\ndef get_notebook_directory() -> Path:\n    '''Best effort to locate the directory that contains this notebook.'''\n    try:\n        import ipynbname\n        nb_path = Path(ipynbname.path()).resolve()\n        if nb_path.exists():\n            return nb_path.parent\n    except Exception:\n        pass\n    try:\n        from IPython import get_ipython\n        ipython = get_ipython()\n        if ipython:\n            cwd = Path(ipython.run_line_magic('pwd', '')).resolve()\n            return cwd\n    except Exception:\n        pass\n    return Path.cwd().resolve()\n\n\ndef resolve_results_dir(dataset: str) -> Path:\n    nb_dir = get_notebook_directory()\n    results_dir = nb_dir / 'results' / dataset\n    results_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"üìÅ Notebook directory: {nb_dir}\")\n    print(f\"üìÅ Results directory: {results_dir}\")\n    return results_dir\n\n\ndef build_base_config(dataset: str, results_dir: Path) -> dict:\n    base = {\n        'n_train_traces': 7,\n        'n_test_traces': None,\n        'train_cases': None,\n        'test_cases': None,\n        'ensure_train_variant_diversity': True,\n        'ensure_test_variant_diversity': True,\n        'use_same_traces_for_train_test': False,\n        'allow_train_cases_in_test': True,\n        'compute_marking_transition_map': False,\n        'sequential_sampling': True,\n        'n_indices': None,\n        'n_per_run': 10000,\n        'independent_sampling': True,\n        'prob_threshold': 1e-6,\n        'chunk_size': 11,\n        'conformance_switch_penalty_weight': 1.0,\n        'merge_mismatched_boundaries': False,\n        'conditioning_combine_fn': linear_prob_combiner,\n        'max_hist_len': 3,\n        'conditioning_n_prev_labels': 3,\n        'use_collapsed_runs': True,\n        'cost_function': \"linear\",\n        'model_move_cost': 1.0,\n        'log_move_cost': 1.0,\n        'tau_move_cost': 0.0,\n        'non_sync_penalty': 1.0,\n        'use_calibration': True,\n        'temp_bounds': (1.0, 10.0),\n        'temperature': None,\n        'verbose': True,\n        'log_level': logging.INFO,\n        'round_precision': 2,\n        'random_seed': 101,\n        'save_model_path': str(results_dir / f'discovered_petri_net_{dataset}'),\n        'save_model': True,\n        'dataset_name': dataset,\n        'parallel_processing': False,\n        'dataset_parallelization': False,\n        'max_workers': MAX_WORKERS,\n    }\n    parallel_modes = {\n        'trace': (True, False),\n        'dataset': (False, True),\n        'both': (False, True),\n        None: (False, False),\n    }\n    base['parallel_processing'], base['dataset_parallelization'] = parallel_modes.get(\n        PARALLELIZATION_STRATEGY, (False, False)\n    )\n    print(\n        f\"üîß Parallelization: {PARALLELIZATION_STRATEGY or 'none'} \"\n        f\"(max_workers={MAX_WORKERS or 'auto'})\"\n    )\n    return base\n\n\nresults_dir = resolve_results_dir(dataset_name)\nbase_config = build_base_config(dataset_name, results_dir)\n\ntrain_trace_values = TRAIN_TRACE_SWEEP.get(dataset_name.lower())\nif not train_trace_values:\n    train_trace_values = [base_config['n_train_traces']]\n    print(\n        f\"‚ö†Ô∏è Dataset '{dataset_name}' not in TRAIN_TRACE_SWEEP; \"\n        f\"using n_train_traces={train_trace_values[-1]}.\"\n    )\nelse:\n    print(f\"üß™ Training-trace sweep: {train_trace_values}\")\n\n# OVERRIDE: Run only n_traces=9 for 50salads (to complete missing run)\nif dataset_name.lower() == '50salads':\n    train_trace_values = [9]\n    MAX_WORKERS = 5  # Reduced from 10 to prevent OOM with complex n=9 model\n    print(f\"üîß OVERRIDE: Running only n_traces=9 for 50salads\")\n    print(f\"üîß OVERRIDE: Trace-level parallelization only (max_workers={MAX_WORKERS})\")\n\nbase_config['n_train_traces'] = train_trace_values[-1]\nbase_config['max_workers'] = MAX_WORKERS\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 12:35:40,090 - src.incremental_softmax_recovery - INFO - Starting incremental softmax recovery (conformance-only).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter grid: 1 train-trace settings √ó 1 alphas √ó 1 strategies = 1 runs\n",
      "Hyperparameter-level parallelization: joblib (n_jobs=1)\n",
      "Run-level parallelization: trace\n",
      "================================================================================\n",
      "[1/1] Running on 50salads: train_traces=9, alpha=0.05, strategy=unigram_super_heavy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 12:35:40,103 - src.incremental_softmax_recovery - INFO - Validated sequential case IDs (found 40 unique cases) and 40 softmax matrices.\n",
      "2025-12-07 12:35:40,104 - src.incremental_softmax_recovery - INFO - Validated sampling parameters: sequential runs with n_per_run=10000.\n",
      "2025-12-07 12:35:40,104 - src.incremental_softmax_recovery - INFO - Validated input parameters: round_precision=2, prob_threshold=1e-06.\n",
      "2025-12-07 12:35:40,104 - src.incremental_softmax_recovery - INFO - Prepared cost function: linear (model=1.0, log=1.0, tau=0.0).\n",
      "2025-12-07 12:35:40,105 - src.incremental_softmax_recovery - INFO - Prepared softmax arrays: 40 traces with individual shape (19, 5687).\n",
      "2025-12-07 12:35:40,648 - src.incremental_softmax_recovery - INFO - Filtered log and softmax matrices: 237820 -> 237820 events (100.0% retained).\n",
      "2025-12-07 12:35:41,159 - src.incremental_softmax_recovery - INFO - Performed train/test split: 9 train cases, 40 test cases.\n",
      "2025-12-07 12:35:41,159 - src.incremental_softmax_recovery - INFO - Train case IDs: ['22', '29', '37', '2', '12', '35', '19', '26', '10']\n",
      "2025-12-07 12:35:41,159 - src.incremental_softmax_recovery - INFO - Test case IDs: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39']\n",
      "2025-12-07 12:35:41,160 - src.incremental_softmax_recovery - INFO - Train/test overlap: 9 case(s) - ['10', '12', '19', '2', '22', '26', '29', '35', '37']\n",
      "2025-12-07 12:35:41,160 - src.incremental_softmax_recovery - INFO - Discovering Petri net model from training data.\n",
      "2025-12-07 12:35:57,515 - src.utils - INFO - PNG visualization saved to: C:\\Users\\User\\Jupyter Projects\\sktr_for_long_traces-1\\results\\50salads\\discovered_petri_net_50salads.png\n",
      "2025-12-07 12:35:57,597 - src.utils - INFO - PDF visualization saved to: C:\\Users\\User\\Jupyter Projects\\sktr_for_long_traces-1\\results\\50salads\\discovered_petri_net_50salads.pdf\n",
      "2025-12-07 12:35:57,600 - src.incremental_softmax_recovery - INFO - Petri net visualization saved to C:\\Users\\User\\Jupyter Projects\\sktr_for_long_traces-1\\results\\50salads\\discovered_petri_net_50salads.pdf\n",
      "2025-12-07 12:35:57,600 - src.incremental_softmax_recovery - INFO - Discovered Petri net model: 75 places, 105 transitions.\n",
      "2025-12-07 12:35:57,600 - src.incremental_softmax_recovery - INFO - Skipping marking-to-transition map computation (compute_marking_transition_map=False).\n",
      "2025-12-07 12:35:57,600 - src.incremental_softmax_recovery - INFO - Disabling lazy map building - will use direct enabled transitions fallback.\n",
      "2025-12-07 12:35:57,713 - src.incremental_softmax_recovery - INFO - Built UNCOLLAPSED probability dictionary (for continuation): 259 histories, avg 1.8 activities per history.\n",
      "2025-12-07 12:35:57,719 - src.incremental_softmax_recovery - INFO - Built COLLAPSED probability dictionary (for transitions): 178 histories, avg 1.6 activities per history.\n",
      "2025-12-07 12:35:58,361 - src.incremental_softmax_recovery - INFO - Prepared 40 test softmax matrices with calibration (temperature=1.51).\n",
      "2025-12-07 12:35:58,367 - src.incremental_softmax_recovery - INFO - Extracted 40 test case IDs for processing.\n",
      "2025-12-07 12:35:58,367 - src.incremental_softmax_recovery - INFO - Processing 40 test cases in parallel (trace-level)\n",
      "2025-12-07 12:35:58,367 - src.incremental_softmax_recovery - INFO - Using 10 trace-level workers (spawn, maxtasksperchild=5)\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter search\n",
    "\n",
    "def run_single_hyperparameter(\n",
    "    n_train_traces,\n",
    "    alpha,\n",
    "    strategy_name,\n",
    "    weights,\n",
    "    idx,\n",
    "    total_runs,\n",
    "    df,\n",
    "    softmax_lst,\n",
    "    base_config,\n",
    "    results_dir,\n",
    "):\n",
    "    dataset_name = base_config.get('dataset_name', 'unknown_dataset')\n",
    "    print(\n",
    "        f\"[{idx}/{total_runs}] Running on {dataset_name}: \"\n",
    "        f\"train_traces={n_train_traces}, alpha={alpha}, strategy={strategy_name}\"\n",
    "    )\n",
    "\n",
    "    run_config = base_config.copy()\n",
    "    run_config['conditioning_alpha'] = alpha\n",
    "    run_config['conditioning_interpolation_weights'] = weights\n",
    "    run_config['n_train_traces'] = n_train_traces\n",
    "\n",
    "    save_model_this_run = run_config.get('save_model', False)\n",
    "    if SAVE_PROCESS_MODEL_ONCE:\n",
    "        save_model_this_run = (idx == 1)\n",
    "    run_config['save_model'] = save_model_this_run\n",
    "    run_config['save_model_path'] = base_config['save_model_path']\n",
    "\n",
    "    run_config.pop('dataset_name', None)\n",
    "\n",
    "    results_df, accuracy_dict, _ = incremental_softmax_recovery(\n",
    "        df=df,\n",
    "        softmax_lst=softmax_lst,\n",
    "        **run_config,\n",
    "    )\n",
    "\n",
    "    csv_filename = (\n",
    "        f\"{dataset_name}_train_{n_train_traces}_hyperparam_search_alpha_{alpha}_\"\n",
    "        f\"weights_{strategy_name}.csv\"\n",
    "    )\n",
    "    csv_path = os.path.join(results_dir, csv_filename)\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    metrics = compute_sktr_vs_argmax_metrics(\n",
    "        csv_path,\n",
    "        case_col='case:concept:name',\n",
    "        sktr_pred_col='sktr_activity',\n",
    "        argmax_pred_col='argmax_activity',\n",
    "        gt_col='ground_truth',\n",
    "        background=0,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'dataset_name': dataset_name,\n",
    "        'n_train_traces': n_train_traces,\n",
    "        'conditioning_alpha': alpha,\n",
    "        'interpolation_weights_strategy': strategy_name,\n",
    "        'interpolation_weights': str(weights),\n",
    "        'sktr_acc_micro': metrics['sktr']['acc_micro'],\n",
    "        'sktr_edit': metrics['sktr']['edit'],\n",
    "        'sktr_f1@10': metrics['sktr']['f1@10'],\n",
    "        'sktr_f1@25': metrics['sktr']['f1@25'],\n",
    "        'sktr_f1@50': metrics['sktr']['f1@50'],\n",
    "        'argmax_acc_micro': metrics['argmax']['acc_micro'],\n",
    "        'argmax_edit': metrics['argmax']['edit'],\n",
    "        'argmax_f1@10': metrics['argmax']['f1@10'],\n",
    "        'argmax_f1@25': metrics['argmax']['f1@25'],\n",
    "        'argmax_f1@50': metrics['argmax']['f1@50'],\n",
    "        'results_csv_path': csv_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# Build parameter grid and run experiments\n",
    "param_combinations = list(product(train_trace_values, alphas, interpolation_strategies.items()))\n",
    "total_runs = len(param_combinations)\n",
    "\n",
    "print(\n",
    "    f\"Starting hyperparameter grid: {len(train_trace_values)} train-trace settings √ó \"\n",
    "    f\"{len(alphas)} alphas √ó {len(interpolation_strategies)} strategies = {total_runs} runs\"\n",
    ")\n",
    "print(f\"Hyperparameter-level parallelization: joblib (n_jobs={HYPERPARAM_N_JOBS})\")\n",
    "print(f\"Run-level parallelization: {PARALLELIZATION_STRATEGY or 'none'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_rows = Parallel(n_jobs=HYPERPARAM_N_JOBS, verbose=10)(\n",
    "    delayed(run_single_hyperparameter)(\n",
    "        n_train_traces,\n",
    "        alpha,\n",
    "        strategy_name,\n",
    "        weights,\n",
    "        idx,\n",
    "        total_runs,\n",
    "        df,\n",
    "        softmax_lst,\n",
    "        base_config,\n",
    "        results_dir,\n",
    "    )\n",
    "    for idx, (n_train_traces, alpha, (strategy_name, weights)) in enumerate(param_combinations, 1)\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Display and save results\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER SEARCH SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Top 10 combinations by SKTR Accuracy:\")\n",
    "display(summary_df.sort_values('sktr_acc_micro', ascending=False).head(10))\n",
    "print()\n",
    "print(\"Full results (sorted by SKTR Accuracy):\")\n",
    "display(summary_df.sort_values('sktr_acc_micro', ascending=False))\n",
    "\n",
    "summary_path = os.path.join(\n",
    "    results_dir,\n",
    "    f\"{base_config['dataset_name']}_hyperparameter_search_summary.csv\",\n",
    ")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print()\n",
    "print(f\"‚úì Summary saved to: {summary_path}\")\n",
    "print(f\"‚úì Total runs completed: {len(summary_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  50SALADS SKTR n_traces results file not found: results/50salads/50salads_sktr_ntraces_results.csv\n",
      "   Make sure the file exists in the results/50salads directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from src.evaluation import compute_sktr_vs_argmax_metrics\n",
    "\n",
    "# Read the SKTR n_traces experiment results for the current dataset\n",
    "# Uses dataset_name from the cell above (gtea, breakfast, or 50salads)\n",
    "ntraces_path = os.path.join('results', dataset_name, f'{dataset_name}_sktr_ntraces_results.csv')\n",
    "\n",
    "try:\n",
    "    ntraces_df = pd.read_csv(ntraces_path)\n",
    "    print(f\"‚úì Loaded {dataset_name.upper()} SKTR n_traces results from: {ntraces_path}\")\n",
    "    print(f\"   Shape: {ntraces_df.shape}\")\n",
    "    print(f\"   Columns: {list(ntraces_df.columns)}\")\n",
    "    \n",
    "    # Display all results from SKTR n_traces experiment (sorted by SKTR Accuracy)\n",
    "    print(f\"\\nAll {dataset_name.upper()} SKTR n_traces results by SKTR Accuracy:\")\n",
    "    display(ntraces_df.sort_values('sktr_acc_micro', ascending=False))\n",
    "    \n",
    "    # Show best n_traces configuration\n",
    "    best_ntraces_row = ntraces_df.loc[ntraces_df['sktr_acc_micro'].idxmax()]\n",
    "    print(f\"\\nBest {dataset_name.upper()} SKTR n_traces configuration:\")\n",
    "    print(f\"   SKTR Accuracy: {best_ntraces_row['sktr_acc_micro']:.4f}\")\n",
    "    print(f\"   n_train_traces: {best_ntraces_row.get('n_train_traces', 'N/A')}\")\n",
    "    print(f\"   alpha: {best_ntraces_row.get('conditioning_alpha', 'N/A')}\")\n",
    "    print(f\"   strategy: {best_ntraces_row.get('interpolation_weights_strategy', 'N/A')}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è  {dataset_name.upper()} SKTR n_traces results file not found: {ntraces_path}\")\n",
    "    print(f\"   Make sure the file exists in the results/{dataset_name} directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error reading {dataset_name.upper()} SKTR n_traces results: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}