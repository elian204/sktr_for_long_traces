{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "from utils import prepare_df, group_cases_by_trace, compute_accuracies_by_case\n",
        "from incremental_softmax_recovery import incremental_softmax_recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging with selective DEBUG for our modules only\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,           # Set root to INFO (reduces third-party noise)\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    force=True                    # Force override of any existing handlers (useful in Jupyter)\n",
        ")\n",
        "\n",
        "# Enable DEBUG for our specific modules only\n",
        "our_modules = [\n",
        "    'classes', \n",
        "    'incremental_softmax_recovery', \n",
        "    'beam_search', \n",
        "    'utils', \n",
        "    'conformance_checking',\n",
        "    'data_processing',\n",
        "    'petri_model',\n",
        "    'calibration'\n",
        "]\n",
        "\n",
        "for module_name in our_modules:\n",
        "    logging.getLogger(module_name).setLevel(logging.DEBUG)\n",
        "\n",
        "# Silence noisy third-party libraries\n",
        "logging.getLogger('graphviz').setLevel(logging.WARNING)  # Only show warnings/errors from graphviz\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)  # Silence matplotlib if present\n",
        "logging.getLogger('PIL').setLevel(logging.WARNING)  # Silence PIL if present\n",
        "\n",
        "print(\"âœ… Logging configured: DEBUG for our modules, INFO+ for third-party libraries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load your DataFrame and softmax list\n",
        "result = prepare_df('50salads')\n",
        "if len(result) == 2:\n",
        "    df, softmax_lst = result\n",
        "else:\n",
        "    df, softmax_lst, _ = result\n",
        "\n",
        "# group by trace and inspect\n",
        "trace_groups = group_cases_by_trace(df)\n",
        "trace_groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated configuration for incremental_softmax_recovery with new parameters\n",
        "config = {\n",
        "    # === Data Splitting ===\n",
        "    'n_train_traces': 10,                     # Number of training traces\n",
        "    'n_test_traces': 1,                     # Number of test traces  \n",
        "    'train_cases': None,                     # Specific train case IDs (overrides n_train_traces)\n",
        "    'test_cases': None,                      # Specific test case IDs (overrides n_test_cases)\n",
        "    'ensure_train_variant_diversity': True,  # Enforce distinct variants in training\n",
        "    'ensure_test_variant_diversity': False,  # Enforce distinct variants in testing\n",
        "    \n",
        "    # === Sampling Configuration ===\n",
        "    'sequential_sampling': True,             # True: sample from activity runs, False: uniform sampling\n",
        "    'n_indices': None,                       # Events to sample per trace (when sequential_sampling=False)\n",
        "    'n_per_run': 10,                          # Events per activity run (when sequential_sampling=True)\n",
        "    'independent_sampling': True,            # Each trace gets different random seed\n",
        "    \n",
        "    # === Recovery Method Selection (NEW!) ===\n",
        "    'recovery_method': 'conformance',        # \"conformance\" or \"beam_search\" - choose your algorithm!\n",
        "    'prob_threshold': 1e-6,                  # Unified threshold for activity filtering (both methods)\n",
        "    \n",
        "    # === Conformance Checking Parameters (NEW!) ===\n",
        "    'chunk_size': 15,                        # Size of chunks for conformance processing\n",
        "    \n",
        "    # === Beam Search Parameters ===\n",
        "    'beam_width': 1,                        # [BEAM SEARCH ONLY] Number of candidates to maintain\n",
        "    'beam_score_alpha': 1.0,                # [BEAM SEARCH ONLY] Weight between avg cost and total cost\n",
        "    'completion_patience': 20,               # [BEAM SEARCH ONLY] Extra iterations after first completion\n",
        "    \n",
        "    # === Cost Function ===\n",
        "    'cost_function': \"linear\",                # \"linear\", \"logarithmic\", or callable\n",
        "    'model_move_cost': 1.0,                   # Cost for model-only moves\n",
        "    'log_move_cost': 1.0,                     # Cost for log-only moves  \n",
        "    'tau_move_cost': 0.0,                     # Cost for silent (tau) moves\n",
        "    'non_sync_penalty': 1.0,                  # Penalty for non-sync moves\n",
        "    'conformance_switch_penalty_weight': 1.0, # Weight for switch penalty in conformance checking\n",
        "    \n",
        "    # === Conditional Probabilities (Beam Search Only) ===\n",
        "    'use_cond_probs': True,                  # [BEAM SEARCH ONLY] Enable conditional probabilities\n",
        "    'max_hist_len': 3,                       # [BEAM SEARCH ONLY] Maximum history length for conditioning\n",
        "    'lambdas': [0.1, 0.3, 0.6],              # [BEAM SEARCH ONLY] Blending weights for n-gram smoothing\n",
        "    'alpha': 0.95,                            # [BEAM SEARCH ONLY] History vs base probability weight (0=history, 1=base)\n",
        "    'use_ngram_smoothing': True,             # [BEAM SEARCH ONLY] Apply n-gram smoothing\n",
        "    \n",
        "    # === Temperature Calibration ===\n",
        "    'use_calibration': True,                 # Enable temperature scaling\n",
        "    'temp_bounds': (1.0, 10.0),              # Temperature optimization bounds\n",
        "    'temperature': None,                     # Manual temperature (bypasses optimization)\n",
        "    \n",
        "    # === Logging ===\n",
        "    'verbose': True,                          # Enable logging output\n",
        "    'log_level': logging.INFO,                # Logging level (logging.DEBUG for more details)\n",
        "    \n",
        "    # === Miscellaneous ===\n",
        "    'round_precision': 2,                     # Decimal places for probability rounding\n",
        "    'random_seed': 101,                       # Random seed for reproducibility\n",
        "    'save_model_path': \"./discovered_petri_net\",  # Path for saved model (without extension)\n",
        "    'save_model': True,                     # Save model to PDF (set to True if you want visualization)\n",
        "}\n",
        "\n",
        "# Usage:\n",
        "output = incremental_softmax_recovery(\n",
        "    df=df,\n",
        "    softmax_lst=softmax_lst,\n",
        "    **config \n",
        ")\n",
        "\n",
        "# Unpack results\n",
        "results_df, accuracy_dict, prob_dict = output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracies = compute_accuracies_by_case(results_df)\n",
        "\n",
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all rows for case '28' without truncation, including full list values\n",
        "case_28_df = results_df[results_df['case:concept:name'] == '11']\n",
        "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None, 'display.width', None):\n",
        "    display(case_28_df)\n",
        "total_cost = case_28_df['sktr_move_cost'].sum()\n",
        "print(f\"Total SKTR move cost for case 28: {total_cost:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "researchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
