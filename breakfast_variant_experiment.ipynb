{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-0",
   "metadata": {},
   "source": "# Breakfast Dataset - Variant Experiment\n\nThis notebook runs experiments on the Breakfast dataset by controlling which trace variants are used for training and testing.\n\n## Notebook Structure\n\n1. **Setup & Data Loading** - Load dataset and analyze variants\n2. **Hyperparameter Search** - Fixed train/test split, sweep over alpha and interpolation strategies\n3. **Final Experiment** - Fixed hyperparameters, sweep over number of training cases"
  },
  {
   "cell_type": "markdown",
   "id": "md-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: /home/dsi/eli-bogdanov/sktr_for_long_traces\n",
      "Running in tmux: False\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Setup workspace path\n",
    "workspace_root = '/home/dsi/eli-bogdanov/sktr_for_long_traces'\n",
    "if workspace_root not in sys.path:\n",
    "    sys.path.insert(0, workspace_root)\n",
    "\n",
    "from src.utils import (\n",
    "    prepare_df, linear_prob_combiner,\n",
    "    get_variant_info, select_variants, get_cases_for_variants, get_variants_for_cases\n",
    ")\n",
    "from src.incremental_softmax_recovery import incremental_softmax_recovery\n",
    "from src.evaluation import compute_sktr_vs_argmax_metrics\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', force=True)\n",
    "for mod in ['src.classes', 'src.incremental_softmax_recovery', 'src.utils', 'src.conformance_checking', 'src.data_processing', 'src.petri_model', 'src.calibration']:\n",
    "    logging.getLogger(mod).setLevel(logging.DEBUG)\n",
    "for mod in ['graphviz', 'matplotlib', 'PIL']:\n",
    "    logging.getLogger(mod).setLevel(logging.WARNING)\n",
    "\n",
    "print(f\"Workspace: {workspace_root}\")\n",
    "print(f\"Running in tmux: {'TMUX' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-2",
   "metadata": {},
   "source": "## 2. Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\n# --- Case Selection ---\n# List of case IDs to use for experiments (order is preserved for sweep)\nCASE_IDS = [649, 357, 542, 834, 1006, 385, 243, 553, 386, 48, 841, 877, 321, 226, 670]\n\n# --- General ---\nRANDOM_SEED = 42\n\n# --- Parallelization ---\nN_PARALLEL_RUNS = 5  # Number of experiment runs to execute in parallel\nDATASET_PARALLELIZATION = True  # Enable parallel processing of test cases\nN_DATASET_WORKERS = 8  # Workers per run for dataset parallelization\n\n# --- Output ---\nSAVE_PROCESS_MODELS = True  # Save discovered Petri net"
  },
  {
   "cell_type": "markdown",
   "id": "md-3",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: breakfast\n",
      "  Events: 2,021,688\n",
      "  Cases: 1008\n",
      "  Activities: 45\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'breakfast'\n",
    "\n",
    "result = prepare_df(dataset_name)\n",
    "df, softmax_lst = result[:2]\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"  Events: {len(df):,}\")\n",
    "print(f\"  Cases: {df['case:concept:name'].nunique()}\")\n",
    "print(f\"  Activities: {df['concept:name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-4",
   "metadata": {},
   "source": "## 4. Prepare Case Data\n\nValidate case IDs and build case-to-variant mapping."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "def get_variant_info(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Analyze trace variants. Returns DataFrame sorted by frequency (variant_id 0 = most frequent).\"\"\"\n    trace_variants = {}\n    for case_id in df['case:concept:name'].unique():\n        trace = tuple(df[df['case:concept:name'] == case_id]['concept:name'].tolist())\n        if trace not in trace_variants:\n            trace_variants[trace] = {'case_ids': [], 'length': len(trace)}\n        trace_variants[trace]['case_ids'].append(case_id)\n    \n    data = [{'variant_id': i, 'trace_signature': sig, 'case_ids': info['case_ids'],\n             'frequency': len(info['case_ids']), 'trace_length': info['length']}\n            for i, (sig, info) in enumerate(trace_variants.items())]\n    \n    variant_df = pd.DataFrame(data).sort_values('frequency', ascending=False).reset_index(drop=True)\n    variant_df['variant_id'] = range(len(variant_df))\n    return variant_df\n\n\ndef get_cases_for_variants(variant_df: pd.DataFrame, variant_ids: List[int], seed: int = 42) -> List[str]:\n    \"\"\"Get all case IDs for the specified variants.\"\"\"\n    cases = []\n    for vid in variant_ids:\n        row = variant_df[variant_df['variant_id'] == vid]\n        if not row.empty:\n            cases.extend(row.iloc[0]['case_ids'])\n    return cases\n\n\nvariant_df = get_variant_info(df)\nprint(f\"Total variants in dataset: {len(variant_df)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Validate case IDs and build mappings\ncase_ids_str = [str(cid) for cid in CASE_IDS]\n\navailable_cases = df['case:concept:name'].unique().astype(str).tolist()\nmissing_cases = [cid for cid in case_ids_str if cid not in available_cases]\nif missing_cases:\n    print(f\"Warning: {len(missing_cases)} case IDs not found: {missing_cases[:10]}...\")\n    case_ids_str = [cid for cid in case_ids_str if cid in available_cases]\n    print(f\"Using {len(case_ids_str)} valid case IDs\")\n\n# Build case_id -> variant_id mapping\ncase_to_variant = {}\nfor _, row in variant_df.iterrows():\n    for cid in row['case_ids']:\n        case_to_variant[str(cid)] = row['variant_id']\n\n# Get variants in list order\nexperiment_variants = []\nseen_variants = set()\nfor cid in case_ids_str:\n    vid = case_to_variant.get(cid)\n    if vid is not None and vid not in seen_variants:\n        experiment_variants.append(vid)\n        seen_variants.add(vid)\n\n# Setup results directory\nresults_dir = Path(workspace_root) / 'results' / dataset_name / 'variant_experiment'\nresults_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Case IDs: {len(case_ids_str)} cases\")\nprint(f\"Unique variants: {len(experiment_variants)} -> {experiment_variants}\")\nprint(f\"\\nCase ID -> Variant mapping:\")\nfor cid in case_ids_str:\n    vid = case_to_variant.get(cid, '?')\n    print(f\"  {cid} -> variant {vid}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-5",
   "metadata": {},
   "source": "## 5. Helper Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "def build_base_config() -> dict:\n    \"\"\"Build base configuration for experiments.\"\"\"\n    return {\n        'n_train_traces': None, 'n_test_traces': None,\n        'train_cases': None, 'test_cases': None,\n        'ensure_train_variant_diversity': False, 'ensure_test_variant_diversity': False,\n        'use_same_traces_for_train_test': False, 'allow_train_cases_in_test': True,\n        'compute_marking_transition_map': False, 'sequential_sampling': True,\n        'n_indices': None, 'n_per_run': 10000, 'independent_sampling': True,\n        'prob_threshold': 1e-6, 'chunk_size': 11, 'conformance_switch_penalty_weight': 1.0,\n        'merge_mismatched_boundaries': False, 'conditioning_combine_fn': linear_prob_combiner,\n        'max_hist_len': 3, 'conditioning_n_prev_labels': 3, 'use_collapsed_runs': True,\n        'cost_function': 'linear', 'model_move_cost': 1.0, 'log_move_cost': 1.0,\n        'tau_move_cost': 0.0, 'non_sync_penalty': 1.0,\n        'use_calibration': True, 'temp_bounds': (1.0, 10.0), 'temperature': None,\n        'verbose': True, 'log_level': logging.INFO, 'round_precision': 2,\n        'random_seed': RANDOM_SEED,\n        'save_model_path': None,\n        'save_model': False,\n        'parallel_processing': False,\n        'dataset_parallelization': DATASET_PARALLELIZATION,\n        'max_workers': N_DATASET_WORKERS,\n    }\n\n\ndef run_single_experiment(train_cases, test_cases, alpha, strategy, weights,\n                          idx, total, df, softmax_lst, base_cfg, results_dir, prefix):\n    \"\"\"Run a single experiment and return metrics.\"\"\"\n    print(f\"[{idx}/{total}] train={len(train_cases)}, test={len(test_cases)}, alpha={alpha}, strategy={strategy}\")\n    \n    cfg = base_cfg.copy()\n    cfg.update({\n        'conditioning_alpha': alpha, \n        'conditioning_interpolation_weights': weights,\n        'train_cases': train_cases, \n        'test_cases': test_cases,\n        'n_train_traces': len(train_cases), \n        'n_test_traces': len(test_cases),\n        'save_model': SAVE_PROCESS_MODELS,\n        'save_model_path': str(results_dir / f'petri_net_{prefix}'),\n    })\n    \n    results_df, _, _ = incremental_softmax_recovery(df=df, softmax_lst=softmax_lst, **cfg)\n    \n    csv_path = results_dir / f\"{dataset_name}_{prefix}_alpha_{alpha}_weights_{strategy}.csv\"\n    results_df.to_csv(csv_path, index=False)\n    \n    metrics = compute_sktr_vs_argmax_metrics(\n        str(csv_path), 'case:concept:name', 'sktr_activity', 'argmax_activity', 'ground_truth', 0\n    )\n    \n    return {\n        'n_train_cases': len(train_cases), 'n_test_cases': len(test_cases),\n        'alpha': alpha, 'strategy': strategy,\n        'sktr_acc': metrics['sktr']['acc_micro'], 'sktr_edit': metrics['sktr']['edit'],\n        'sktr_f1@10': metrics['sktr']['f1@10'], 'sktr_f1@25': metrics['sktr']['f1@25'], 'sktr_f1@50': metrics['sktr']['f1@50'],\n        'argmax_acc': metrics['argmax']['acc_micro'], 'argmax_edit': metrics['argmax']['edit'],\n        'argmax_f1@10': metrics['argmax']['f1@10'], 'argmax_f1@25': metrics['argmax']['f1@25'], 'argmax_f1@50': metrics['argmax']['f1@50'],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "base_config = build_base_config()\nprint(\"Base config ready.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-6",
   "metadata": {},
   "source": "---\n\n# Part A: Hyperparameter Search\n\nFixed train/test split, sweep over all alpha and interpolation strategy combinations to find optimal hyperparameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# HYPERPARAMETER SEARCH CONFIGURATION\n# =============================================================================\n\n# Train/test split for hyperparameter search (use all cases for both)\nHP_TRAIN_CASES = case_ids_str  # All cases for training\nHP_TEST_CASES = case_ids_str   # All cases for testing\n\n# Hyperparameter grid to search\nHP_ALPHAS = [0.01, 0.05, 0.1, 0.2]\nHP_STRATEGIES = {\n    'unigram_super_heavy': [0.75, 0.15, 0.1],\n    'balanced': [0.33, 0.33, 0.34],\n    'bigram_heavy': [0.2, 0.6, 0.2],\n}\n\nprint(f\"Hyperparameter Search Setup:\")\nprint(f\"  Train cases: {len(HP_TRAIN_CASES)}\")\nprint(f\"  Test cases: {len(HP_TEST_CASES)}\")\nprint(f\"  Alphas: {HP_ALPHAS}\")\nprint(f\"  Strategies: {list(HP_STRATEGIES.keys())}\")\nprint(f\"  Total experiments: {len(HP_ALPHAS) * len(HP_STRATEGIES)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Run hyperparameter search\nhp_results_dir = results_dir / 'hyperparameter_search'\nhp_results_dir.mkdir(parents=True, exist_ok=True)\n\nhp_params = [(HP_TRAIN_CASES, HP_TEST_CASES, a, s, w)\n             for a in HP_ALPHAS\n             for s, w in HP_STRATEGIES.items()]\n\nn_jobs = N_PARALLEL_RUNS if N_PARALLEL_RUNS is not None else -1\n\nprint(f\"Running {len(hp_params)} hyperparameter experiments...\")\nprint(\"=\" * 60)\n\nhp_results = Parallel(n_jobs=n_jobs, verbose=10)(\n    delayed(run_single_experiment)(\n        train, test, alpha, strategy, weights,\n        i, len(hp_params), df, softmax_lst, base_config, hp_results_dir, f\"hp_search\"\n    )\n    for i, (train, test, alpha, strategy, weights) in enumerate(hp_params, 1)\n)\n\nhp_summary_df = pd.DataFrame(hp_results).sort_values('sktr_acc', ascending=False)\nhp_summary_path = hp_results_dir / f\"{dataset_name}_hp_search_summary.csv\"\nhp_summary_df.to_csv(hp_summary_path, index=False)\nprint(f\"\\nSaved: {hp_summary_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-7",
   "metadata": {},
   "source": "### Hyperparameter Search Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Display hyperparameter search results (sorted by SKTR accuracy)\nprint(\"Hyperparameter Search Results (sorted by SKTR accuracy):\\n\")\ndisplay(hp_summary_df[['alpha', 'strategy', 'sktr_acc', 'argmax_acc', 'sktr_edit', 'argmax_edit', 'sktr_f1@25', 'argmax_f1@25']])\n\n# Best hyperparameters\nbest_hp = hp_summary_df.iloc[0]\nprint(f\"\\nBest hyperparameters:\")\nprint(f\"  Alpha: {best_hp['alpha']}\")\nprint(f\"  Strategy: {best_hp['strategy']}\")\nprint(f\"  SKTR Accuracy: {best_hp['sktr_acc']:.4f}\")\nprint(f\"  SKTR Edit: {best_hp['sktr_edit']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize hyperparameter search results\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nfor ax, metric in zip(axes, ['sktr_acc', 'sktr_edit', 'sktr_f1@25']):\n    pivot = hp_summary_df.pivot(index='alpha', columns='strategy', values=metric)\n    pivot.plot(kind='bar', ax=ax, rot=0)\n    ax.set_xlabel('Alpha')\n    ax.set_ylabel(metric.replace('sktr_', '').replace('_', ' ').title())\n    ax.set_title(f'SKTR {metric.replace(\"sktr_\", \"\").replace(\"_\", \" \").title()}')\n    ax.legend(title='Strategy', fontsize=8)\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.suptitle('Hyperparameter Search Results', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(hp_results_dir / f'{dataset_name}_hp_search_plots.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "---\n\n# Part B: Final Experiment (Training Sweep)\n\nUse the best hyperparameters from Part A and sweep over number of training cases to study the effect of training data size."
  },
  {
   "cell_type": "code",
   "id": "6289hrcdtpb",
   "source": "# =============================================================================\n# FINAL EXPERIMENT CONFIGURATION\n# =============================================================================\n\n# Option 1: Use best hyperparameters from search (uncomment to use)\n# FINAL_ALPHA = best_hp['alpha']\n# FINAL_STRATEGY = best_hp['strategy']\n# FINAL_WEIGHTS = HP_STRATEGIES[best_hp['strategy']]\n\n# Option 2: Manually specify hyperparameters\nFINAL_ALPHA = 0.05\nFINAL_STRATEGY = 'unigram_super_heavy'\nFINAL_WEIGHTS = [0.75, 0.15, 0.1]\n\n# Training sweep configuration\nSWEEP_STEP = 1  # Step size: 1 = all values (1,2,3,...,N), 5 = every 5th (1,6,11,...)\nSWEEP_START = 1  # Starting number of training cases\n\n# Test set (always all cases)\nFINAL_TEST_CASES = case_ids_str\n\n# Generate training sweep values\nn_total = len(case_ids_str)\nsweep_values = list(range(SWEEP_START, n_total + 1, SWEEP_STEP))\nif n_total not in sweep_values:  # Always include the final value\n    sweep_values.append(n_total)\n\nprint(f\"Final Experiment Configuration:\")\nprint(f\"  Alpha: {FINAL_ALPHA}\")\nprint(f\"  Strategy: {FINAL_STRATEGY}\")\nprint(f\"  Weights: {FINAL_WEIGHTS}\")\nprint(f\"  Test cases: {len(FINAL_TEST_CASES)} (all)\")\nprint(f\"  Training sweep: {sweep_values}\")\nprint(f\"  Total experiments: {len(sweep_values)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tpcmdeal8zj",
   "source": "# Build training configurations for sweep\nsweep_configs = {}\nfor n in sweep_values:\n    train_case_ids = case_ids_str[:n]\n    # Find which variants these cases belong to\n    train_variant_ids = []\n    seen = set()\n    for cid in train_case_ids:\n        vid = case_to_variant.get(cid)\n        if vid is not None and vid not in seen:\n            train_variant_ids.append(vid)\n            seen.add(vid)\n    sweep_configs[n] = {\n        'variant_ids': train_variant_ids,\n        'case_ids': train_case_ids\n    }\n\nprint(\"Training Sweep Progression:\")\nfor n, cfg in sweep_configs.items():\n    cases_str = str(cfg['case_ids']) if len(cfg['case_ids']) <= 5 else f\"{cfg['case_ids'][:3]}...{cfg['case_ids'][-1]}\"\n    print(f\"  n={n:2d} -> {len(cfg['case_ids']):2d} case(s), {len(cfg['variant_ids']):2d} variant(s)  cases={cases_str}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qp7lni9l5xf",
   "source": "# Run final experiment sweep\nfinal_results_dir = results_dir / 'final_experiment'\nfinal_results_dir.mkdir(parents=True, exist_ok=True)\n\nsweep_params = [(cfg['case_ids'], FINAL_TEST_CASES, FINAL_ALPHA, FINAL_STRATEGY, FINAL_WEIGHTS, n)\n                for n, cfg in sweep_configs.items()]\n\nprint(f\"Running {len(sweep_params)} sweep experiments...\")\nprint(\"=\" * 60)\n\nsweep_results = Parallel(n_jobs=n_jobs, verbose=10)(\n    delayed(run_single_experiment)(\n        train, test, alpha, strategy, weights,\n        i, len(sweep_params), df, softmax_lst, base_config, final_results_dir, f\"sweep_n{n}\"\n    )\n    for i, (train, test, alpha, strategy, weights, n) in enumerate(sweep_params, 1)\n)\n\n# Add n_train to results and variant info\nfor i, (n, cfg) in enumerate(sweep_configs.items()):\n    sweep_results[i]['n_train'] = n\n    sweep_results[i]['n_variants'] = len(cfg['variant_ids'])\n\nsweep_summary_df = pd.DataFrame(sweep_results).sort_values('n_train')\nsweep_summary_path = final_results_dir / f\"{dataset_name}_sweep_summary.csv\"\nsweep_summary_df.to_csv(sweep_summary_path, index=False)\nprint(f\"\\nSaved: {sweep_summary_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ggptwcy87ml",
   "source": "### Final Experiment Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kq0zo7pg4gf",
   "source": "# Display sweep results\nprint(\"Training Sweep Results:\\n\")\ndisplay(sweep_summary_df[['n_train', 'n_variants', 'sktr_acc', 'argmax_acc', 'sktr_edit', 'argmax_edit', 'sktr_f1@25', 'argmax_f1@25']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w6vgu46p7g",
   "source": "# Visualization of sweep results\nfig, axes = plt.subplots(2, 2, figsize=(12, 9))\nx = sweep_summary_df['n_train']\n\nfor ax, (metric, title) in zip(axes.flat[:3], [('acc', 'Accuracy'), ('edit', 'Edit Score'), ('f1@25', 'F1@25')]):\n    ax.plot(x, sweep_summary_df[f'sktr_{metric}'], 'b-o', label='SKTR', lw=2, ms=6)\n    ax.plot(x, sweep_summary_df[f'argmax_{metric}'], 'r--s', label='Argmax', lw=2, ms=6)\n    ax.set_xlabel('Training Cases')\n    ax.set_ylabel(title)\n    ax.set_title(title)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\naxes[1, 1].bar(range(len(x)), sweep_summary_df['n_variants'], color='steelblue', alpha=0.7)\naxes[1, 1].set_xlabel('Training Cases')\naxes[1, 1].set_ylabel('Variants')\naxes[1, 1].set_title('Number of Variants')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\naxes[1, 1].set_xticks(range(len(x)))\naxes[1, 1].set_xticklabels(x)\n\nplt.suptitle(f'Training Sweep (Î±={FINAL_ALPHA}, {FINAL_STRATEGY})', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(final_results_dir / f'{dataset_name}_sweep_plots.png', dpi=150)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6scsh8iinn",
   "source": "# Improvement analysis\nanalysis = sweep_summary_df.copy()\nanalysis['acc_gain'] = analysis['sktr_acc'] - analysis['argmax_acc']\nanalysis['edit_gain'] = analysis['sktr_edit'] - analysis['argmax_edit']\nanalysis['f1@25_gain'] = analysis['sktr_f1@25'] - analysis['argmax_f1@25']\n\nprint(\"SKTR Improvement over Argmax:\")\nprint(f\"  Accuracy:  mean={analysis['acc_gain'].mean():+.4f}, max={analysis['acc_gain'].max():+.4f}\")\nprint(f\"  Edit:      mean={analysis['edit_gain'].mean():+.4f}, max={analysis['edit_gain'].max():+.4f}\")\nprint(f\"  F1@25:     mean={analysis['f1@25_gain'].mean():+.4f}, max={analysis['f1@25_gain'].max():+.4f}\")\n\nbest_idx = analysis['sktr_acc'].idxmax()\nprint(f\"\\nBest SKTR accuracy: {analysis.loc[best_idx, 'sktr_acc']:.4f} at n_train={analysis.loc[best_idx, 'n_train']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}