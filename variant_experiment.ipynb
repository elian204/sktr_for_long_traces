{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-0",
   "metadata": {},
   "source": [
    "# Unified Variant Experiment\n",
    "\n",
    "This notebook runs experiments on video activity segmentation datasets by controlling which trace variants are used for training and testing.\n",
    "\n",
    "**Supported Datasets:** 50salads, gtea, breakfast\n",
    "\n",
    "**Model Sources:** ASFormer, MS-TCN2, Original (pickle files)\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Setup & Data Loading** - Load dataset and analyze variants\n",
    "2. **Hyperparameter Search** - Fixed train/test split, sweep over alpha and interpolation strategies\n",
    "3. **Final Experiment** - Fixed hyperparameters, sweep over number of training variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nimport math\nimport time\nimport logging\nfrom pathlib import Path\nfrom typing import List\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom joblib import Parallel, delayed\n\n# Setup workspace path\nworkspace_root = '/home/dsi/eli-bogdanov/sktr_for_long_traces'\nif workspace_root not in sys.path:\n    sys.path.insert(0, workspace_root)\n\nfrom src.utils import (\n    prepare_df, prepare_df_from_model, linear_prob_combiner,\n    get_variant_info, get_cases_for_variants\n)\nfrom src.incremental_softmax_recovery import incremental_softmax_recovery\nfrom src.evaluation import compute_sktr_vs_argmax_metrics\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', force=True)\nfor mod in ['src.classes', 'src.incremental_softmax_recovery', 'src.utils', 'src.conformance_checking', 'src.data_processing', 'src.petri_model', 'src.calibration']:\n    logging.getLogger(mod).setLevel(logging.DEBUG)\nfor mod in ['graphviz', 'matplotlib', 'PIL']:\n    logging.getLogger(mod).setLevel(logging.WARNING)\n\nprint(f\"Workspace: {workspace_root}\")\nprint(f\"Running in tmux: {'TMUX' in os.environ}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md-2",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Edit this cell to select your dataset and model source.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these values\n",
    "# =============================================================================\n",
    "\n",
    "# --- Dataset Selection ---\n",
    "# Options: '50salads', 'gtea', 'breakfast'\n",
    "DATASET_NAME = '50salads'\n",
    "\n",
    "# --- Model Source ---\n",
    "# Options: 'asformer', 'mstcn2', 'original' (pickle files)\n",
    "MODEL_SOURCE = 'asformer'\n",
    "\n",
    "# --- General ---\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- Parallelization ---\n",
    "N_PARALLEL_RUNS = 1  # Sequential hyperparameter experiments\n",
    "DATASET_PARALLELIZATION = True\n",
    "N_DATASET_WORKERS = 10  # Workers for dataset parallelization\n",
    "\n",
    "# --- Output ---\n",
    "SAVE_PROCESS_MODELS = True\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET_NAME}\")\n",
    "print(f\"  Model Source: {MODEL_SOURCE}\")\n",
    "print(f\"  Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"  Parallelization: {N_DATASET_WORKERS} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-3",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name: str, model_source: str):\n",
    "    \"\"\"Load dataset from specified source.\"\"\"\n",
    "    print(f\"Loading {dataset_name} from {model_source}...\")\n",
    "    \n",
    "    if model_source == 'original':\n",
    "        result = prepare_df(dataset_name)\n",
    "        df, softmax_lst = result[:2]\n",
    "    else:\n",
    "        df, softmax_lst = prepare_df_from_model(dataset_name, model_source)\n",
    "    \n",
    "    print(f\"  Loaded {len(softmax_lst)} cases, {len(df)} events\")\n",
    "    return df, softmax_lst\n",
    "\n",
    "df, softmax_lst = load_dataset(DATASET_NAME, MODEL_SOURCE)\n",
    "\n",
    "print(f\"\\nDataset: {DATASET_NAME}\")\n",
    "print(f\"  Events: {len(df):,}\")\n",
    "print(f\"  Cases: {df['case:concept:name'].nunique()}\")\n",
    "print(f\"  Activities: {df['concept:name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-4",
   "metadata": {},
   "source": [
    "## 4. Analyze Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_df = get_variant_info(df)\n",
    "n_unique_variants = len(variant_df)\n",
    "print(f\"Total unique variants in dataset: {n_unique_variants}\")\n",
    "\n",
    "# Display variant distribution\n",
    "print(f\"\\nTop 10 most frequent variants:\")\n",
    "display(variant_df[['variant_id', 'frequency', 'trace_length']].head(10))\n",
    "\n",
    "# Setup results directory\n",
    "results_dir = Path(workspace_root) / 'results' / DATASET_NAME / 'variant_experiment' / MODEL_SOURCE\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nResults directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-5",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_config() -> dict:\n",
    "    \"\"\"Build base configuration for experiments.\"\"\"\n",
    "    return {\n",
    "        'n_train_traces': None, 'n_test_traces': None,\n",
    "        'train_cases': None, 'test_cases': None,\n",
    "        'ensure_train_variant_diversity': True,\n",
    "        'ensure_test_variant_diversity': True,\n",
    "        'use_same_traces_for_train_test': False, 'allow_train_cases_in_test': True,\n",
    "        'compute_marking_transition_map': False, 'sequential_sampling': True,\n",
    "        'n_indices': None, 'n_per_run': 10000, 'independent_sampling': True,\n",
    "        'prob_threshold': 1e-6, 'chunk_size': 11, 'conformance_switch_penalty_weight': 1.0,\n",
    "        'merge_mismatched_boundaries': False, 'conditioning_combine_fn': linear_prob_combiner,\n",
    "        'max_hist_len': 3, 'conditioning_n_prev_labels': 3, 'use_collapsed_runs': True,\n",
    "        'cost_function': 'linear', 'model_move_cost': 1.0, 'log_move_cost': 1.0,\n",
    "        'tau_move_cost': 0.0, 'non_sync_penalty': 1.0,\n",
    "        'use_calibration': True, 'temp_bounds': (1.0, 10.0), 'temperature': None,\n",
    "        'verbose': True, 'log_level': logging.INFO, 'round_precision': 2,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'save_model_path': None, 'save_model': False,\n",
    "        'parallel_processing': False,\n",
    "        'dataset_parallelization': DATASET_PARALLELIZATION,\n",
    "        'max_workers': N_DATASET_WORKERS,\n",
    "        'conditioning_state_mode': 'topm', 'conditioning_top_m': 3,\n",
    "        'candidate_top_p': 0.9, 'candidate_top_k': 15, 'candidate_min_k': 1,\n",
    "        'candidate_source': 'observed', 'candidate_apply_to_sync': True,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_single_experiment(n_train_traces, test_cases, alpha, strategy, weights,\n",
    "                          idx, total, df, softmax_lst, base_cfg, results_dir, prefix):\n",
    "    \"\"\"Run a single experiment and return metrics including timing info.\"\"\"\n",
    "    print(f\"[{idx}/{total}] n_train_traces={n_train_traces}, alpha={alpha}, strategy={strategy}\")\n",
    "    \n",
    "    cfg = base_cfg.copy()\n",
    "    cfg.update({\n",
    "        'conditioning_alpha': alpha,\n",
    "        'conditioning_interpolation_weights': weights,\n",
    "        'n_train_traces': n_train_traces,\n",
    "        'test_cases': test_cases,\n",
    "        'n_test_traces': len(test_cases) if test_cases else None,\n",
    "        'save_model': SAVE_PROCESS_MODELS,\n",
    "        'save_model_path': str(results_dir / f'petri_net_{prefix}'),\n",
    "    })\n",
    "    \n",
    "    # Time the recovery process\n",
    "    start_time = time.time()\n",
    "    results_df, _, _ = incremental_softmax_recovery(df=df, softmax_lst=softmax_lst, **cfg)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    n_test = results_df['case:concept:name'].nunique()\n",
    "    avg_time_per_trace = total_time / n_test if n_test > 0 else 0\n",
    "    \n",
    "    csv_path = results_dir / f\"{DATASET_NAME}_{MODEL_SOURCE}_{prefix}_alpha_{alpha}_weights_{strategy}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    metrics = compute_sktr_vs_argmax_metrics(\n",
    "        str(csv_path),\n",
    "        case_col='case:concept:name',\n",
    "        sktr_pred_col='sktr_activity',\n",
    "        argmax_pred_col='argmax_activity',\n",
    "        gt_col='ground_truth',\n",
    "        background=None,\n",
    "        dataset_name=DATASET_NAME,\n",
    "    )\n",
    "    \n",
    "    print(f\"  -> Time: {total_time:.1f}s total, {avg_time_per_trace:.2f}s/trace ({n_test} traces)\")\n",
    "    \n",
    "    return {\n",
    "        'n_train_traces': n_train_traces, 'n_test_cases': len(test_cases) if test_cases else 'all',\n",
    "        'alpha': alpha, 'strategy': strategy,\n",
    "        'sktr_acc': metrics['sktr']['acc_micro'], 'sktr_edit': metrics['sktr']['edit'],\n",
    "        'sktr_f1@10': metrics['sktr']['f1@10'], 'sktr_f1@25': metrics['sktr']['f1@25'],\n",
    "        'sktr_f1@50': metrics['sktr']['f1@50'],\n",
    "        'argmax_acc': metrics['argmax']['acc_micro'], 'argmax_edit': metrics['argmax']['edit'],\n",
    "        'argmax_f1@10': metrics['argmax']['f1@10'], 'argmax_f1@25': metrics['argmax']['f1@25'],\n",
    "        'argmax_f1@50': metrics['argmax']['f1@50'],\n",
    "        'total_time_sec': round(total_time, 2), 'avg_time_per_trace_sec': round(avg_time_per_trace, 3),\n",
    "    }\n",
    "\n",
    "base_config = build_base_config()\n",
    "print(\"Base config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Hyperparameter Search\n",
    "\n",
    "Use all unique variants for training, sweep over alpha and interpolation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search configuration\n",
    "HP_N_TRAIN_TRACES = n_unique_variants  # Use all unique variants\n",
    "\n",
    "HP_ALPHAS = [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n",
    "HP_STRATEGIES = {\n",
    "    'unigram_super_heavy': [0.75, 0.15, 0.1],\n",
    "    'balanced': [0.33, 0.33, 0.34],\n",
    "    'bigram_heavy': [0.2, 0.6, 0.2],\n",
    "    'trigram_heavy': [0.1, 0.15, 0.75],\n",
    "}\n",
    "\n",
    "print(f\"Hyperparameter Search Setup:\")\n",
    "print(f\"  Training traces (unique variants): {HP_N_TRAIN_TRACES}\")\n",
    "print(f\"  Test cases: all\")\n",
    "print(f\"  Alphas: {HP_ALPHAS}\")\n",
    "print(f\"  Strategies: {list(HP_STRATEGIES.keys())}\")\n",
    "print(f\"  Total experiments: {len(HP_ALPHAS) * len(HP_STRATEGIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter search\n",
    "hp_results_dir = results_dir / 'hyperparameter_search'\n",
    "hp_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "hp_params = [(HP_N_TRAIN_TRACES, None, a, s, w)\n",
    "             for a in HP_ALPHAS\n",
    "             for s, w in HP_STRATEGIES.items()]\n",
    "\n",
    "n_jobs = N_PARALLEL_RUNS if N_PARALLEL_RUNS is not None else -1\n",
    "\n",
    "print(f\"Running {len(hp_params)} hyperparameter experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hp_results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "    delayed(run_single_experiment)(\n",
    "        n_train, test, alpha, strategy, weights,\n",
    "        i, len(hp_params), df, softmax_lst, base_config, hp_results_dir, \"hp_search\"\n",
    "    )\n",
    "    for i, (n_train, test, alpha, strategy, weights) in enumerate(hp_params, 1)\n",
    ")\n",
    "\n",
    "hp_summary_df = pd.DataFrame(hp_results).sort_values('sktr_acc', ascending=False)\n",
    "hp_summary_path = hp_results_dir / f\"{DATASET_NAME}_{MODEL_SOURCE}_hp_search_summary.csv\"\n",
    "hp_summary_df.to_csv(hp_summary_path, index=False)\n",
    "print(f\"\\nSaved: {hp_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-7",
   "metadata": {},
   "source": [
    "### Hyperparameter Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter search results\n",
    "print(\"Hyperparameter Search Results (sorted by SKTR accuracy):\\n\")\n",
    "display(hp_summary_df[['alpha', 'strategy', 'sktr_acc', 'argmax_acc', 'sktr_edit', 'argmax_edit', 'sktr_f1@25', 'argmax_f1@25']])\n",
    "\n",
    "# Best hyperparameters\n",
    "best_hp = hp_summary_df.iloc[0]\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "print(f\"  Alpha: {best_hp['alpha']}\")\n",
    "print(f\"  Strategy: {best_hp['strategy']}\")\n",
    "print(f\"  SKTR Accuracy: {best_hp['sktr_acc']:.4f}\")\n",
    "print(f\"  SKTR Edit: {best_hp['sktr_edit']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter search results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "plot_cols = ['sktr_acc', 'sktr_edit', 'sktr_f1@25']\n",
    "y_min = hp_summary_df[plot_cols].min().min()\n",
    "y_max = hp_summary_df[plot_cols].max().max()\n",
    "y_lower = math.floor(y_min / 10) * 10\n",
    "y_max = max(y_max, 80)\n",
    "tick_start = y_lower\n",
    "tick_end = math.ceil(y_max / 10) * 10\n",
    "y_limits = (tick_start, tick_end)\n",
    "y_ticks = list(range(int(tick_start), int(tick_end) + 1, 10))\n",
    "\n",
    "for ax, metric in zip(axes, plot_cols):\n",
    "    pivot = hp_summary_df.pivot(index='alpha', columns='strategy', values=metric)\n",
    "    pivot.plot(kind='bar', ax=ax, rot=0)\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_ylabel(metric.replace('sktr_', '').replace('_', ' ').title())\n",
    "    ax.set_title(f'SKTR {metric.replace(\"sktr_\", \"\").replace(\"_\", \" \").title()}')\n",
    "    ax.legend(title='Strategy', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    if y_limits is not None:\n",
    "        ax.set_ylim(*y_limits)\n",
    "    if y_ticks is not None:\n",
    "        ax.set_yticks(y_ticks)\n",
    "\n",
    "plt.suptitle(f'Hyperparameter Search Results ({DATASET_NAME} - {MODEL_SOURCE})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(hp_results_dir / f'{DATASET_NAME}_{MODEL_SOURCE}_hp_search_plots.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Final Experiment (Training Sweep)\n",
    "\n",
    "Use the best hyperparameters from Part A and sweep over number of training variants (1 to N unique variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Final experiment configuration - use best hyperparameters from search\nFINAL_ALPHA = best_hp['alpha']\nFINAL_STRATEGY = best_hp['strategy']\nFINAL_WEIGHTS = HP_STRATEGIES[best_hp['strategy']]\n\n# Predefined sweep ranges per dataset (based on unique variants in ground truth)\n# GTEA: 28 videos, 28 unique variants\n# 50salads: 50 videos, 50 unique variants\n# Breakfast: 1712 videos, 267 unique variants (TBD)\nTRAIN_TRACE_SWEEP = {\n    '50salads': [1, 5, 10, 20, 30, 40, 50],  # 50 unique variants\n    'gtea': [1, 5, 10, 15, 20, 28],           # 28 unique variants\n    'breakfast': list(range(1, 15)),          # TBD - waiting for ASFormer predictions\n}\n\nsweep_values = TRAIN_TRACE_SWEEP.get(DATASET_NAME, list(range(1, n_unique_variants + 1)))\n\nprint(f\"Final Experiment Configuration:\")\nprint(f\"  Alpha: {FINAL_ALPHA}\")\nprint(f\"  Strategy: {FINAL_STRATEGY}\")\nprint(f\"  Weights: {FINAL_WEIGHTS}\")\nprint(f\"  Test cases: all\")\nprint(f\"  Training sweep: {sweep_values}\")\nprint(f\"  Total experiments: {len(sweep_values)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run final experiment sweep\n",
    "final_results_dir = results_dir / 'final_experiment'\n",
    "final_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sweep_params = [(n, None, FINAL_ALPHA, FINAL_STRATEGY, FINAL_WEIGHTS)\n",
    "                for n in sweep_values]\n",
    "\n",
    "print(f\"Running {len(sweep_params)} sweep experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sweep_results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "    delayed(run_single_experiment)(\n",
    "        n_train, test, alpha, strategy, weights,\n",
    "        i, len(sweep_params), df, softmax_lst, base_config, final_results_dir, f\"sweep_n{n_train}\"\n",
    "    )\n",
    "    for i, (n_train, test, alpha, strategy, weights) in enumerate(sweep_params, 1)\n",
    ")\n",
    "\n",
    "sweep_summary_df = pd.DataFrame(sweep_results).sort_values('n_train_traces')\n",
    "sweep_summary_path = final_results_dir / f\"{DATASET_NAME}_{MODEL_SOURCE}_sweep_summary.csv\"\n",
    "sweep_summary_df.to_csv(sweep_summary_path, index=False)\n",
    "print(f\"\\nSaved: {sweep_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-9",
   "metadata": {},
   "source": [
    "### Final Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Display sweep results\nprint(\"Training Sweep Results:\\n\")\ndisplay_cols = ['n_train_traces', 'sktr_acc', 'argmax_acc', 'sktr_edit', 'argmax_edit', 'sktr_f1@25', 'argmax_f1@25']\nif 'avg_time_per_trace_sec' in sweep_summary_df.columns:\n    display_cols.append('avg_time_per_trace_sec')\ndisplay(sweep_summary_df[display_cols])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Improvement analysis\nanalysis = sweep_summary_df.copy()\nanalysis['acc_gain'] = analysis['sktr_acc'] - analysis['argmax_acc']\nanalysis['edit_gain'] = analysis['sktr_edit'] - analysis['argmax_edit']\nanalysis['f1@25_gain'] = analysis['sktr_f1@25'] - analysis['argmax_f1@25']\n\nprint(\"SKTR Improvement over Argmax:\")\nprint(f\"  Accuracy:  mean={analysis['acc_gain'].mean():+.4f}, max={analysis['acc_gain'].max():+.4f}\")\nprint(f\"  Edit:      mean={analysis['edit_gain'].mean():+.4f}, max={analysis['edit_gain'].max():+.4f}\")\nprint(f\"  F1@25:     mean={analysis['f1@25_gain'].mean():+.4f}, max={analysis['f1@25_gain'].max():+.4f}\")\n\n# Timing summary\nif 'avg_time_per_trace_sec' in sweep_summary_df.columns:\n    valid_times = sweep_summary_df['avg_time_per_trace_sec'].dropna()\n    if len(valid_times) > 0:\n        print(f\"\\nTiming Summary:\")\n        print(f\"  Avg time per trace: {valid_times.mean():.3f}s (min={valid_times.min():.3f}s, max={valid_times.max():.3f}s)\")\n        total_times = sweep_summary_df['total_time_sec'].dropna()\n        if len(total_times) > 0:\n            print(f\"  Total experiment time: {total_times.sum():.1f}s ({total_times.sum()/60:.1f} min)\")\n\nbest_idx = analysis['sktr_acc'].idxmax()\nprint(f\"\\nBest SKTR accuracy: {analysis.loc[best_idx, 'sktr_acc']:.4f} at n_train_traces={analysis.loc[best_idx, 'n_train_traces']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of sweep results\n",
    "sns.set_theme(style='whitegrid', context='notebook', palette='deep')\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_config = [\n",
    "    ('acc', 'Accuracy'),\n",
    "    ('edit', 'Edit Score'),\n",
    "    ('f1@10', 'F1@10'),\n",
    "    ('f1@25', 'F1@25'),\n",
    "    ('f1@50', 'F1@50'),\n",
    "]\n",
    "\n",
    "method_styles = {\n",
    "    'sktr': {'color': '#1f77b4', 'marker': 'o', 'label': 'SKTR', 'linestyle': '-'},\n",
    "    'argmax': {'color': '#ff7f0e', 'marker': 's', 'label': 'Argmax', 'linestyle': '--'},\n",
    "}\n",
    "\n",
    "# Collect all plot columns for y-axis scaling\n",
    "plot_cols = []\n",
    "for metric_suffix, _ in metrics_config:\n",
    "    for method in method_styles:\n",
    "        col_name = f'{method}_{metric_suffix}'\n",
    "        if col_name in sweep_summary_df.columns:\n",
    "            plot_cols.append(col_name)\n",
    "\n",
    "# Compute y-axis limits\n",
    "y_limits = None\n",
    "y_ticks = None\n",
    "if plot_cols:\n",
    "    y_min = sweep_summary_df[plot_cols].min().min()\n",
    "    y_max = sweep_summary_df[plot_cols].max().max()\n",
    "    y_lower = math.floor(y_min / 10) * 10\n",
    "    y_max = max(y_max, 80)\n",
    "    tick_start = y_lower\n",
    "    tick_end = math.ceil(y_max / 10) * 10\n",
    "    y_limits = (tick_start, tick_end)\n",
    "    y_ticks = list(range(int(tick_start), int(tick_end) + 1, 10))\n",
    "\n",
    "for idx, (metric_suffix, title) in enumerate(metrics_config):\n",
    "    ax = axes[idx]\n",
    "    for method, style in method_styles.items():\n",
    "        col_name = f'{method}_{metric_suffix}'\n",
    "        if col_name in sweep_summary_df.columns:\n",
    "            sns.lineplot(\n",
    "                x=sweep_summary_df['n_train_traces'],\n",
    "                y=sweep_summary_df[col_name],\n",
    "                ax=ax,\n",
    "                linewidth=2.5,\n",
    "                markersize=9,\n",
    "                **style,\n",
    "            )\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Training Variants')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xticks(sweep_summary_df['n_train_traces'].unique())\n",
    "    if y_limits is not None:\n",
    "        ax.set_ylim(*y_limits)\n",
    "    if y_ticks is not None:\n",
    "        ax.set_yticks(y_ticks)\n",
    "    ax.legend().remove()\n",
    "\n",
    "# Legend in last subplot\n",
    "ax_legend = axes[5]\n",
    "ax_legend.axis('off')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "ax_legend.legend(\n",
    "    handles, labels,\n",
    "    loc='center', title='Method',\n",
    "    fontsize=14, title_fontsize=16,\n",
    "    frameon=True, fancybox=True, shadow=True,\n",
    ")\n",
    "\n",
    "plt.suptitle(f'Performance vs. Training Variants ({DATASET_NAME} - {MODEL_SOURCE})', fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(final_results_dir / f'{DATASET_NAME}_{MODEL_SOURCE}_sweep_plots.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Saved Results (Standalone)\n",
    "\n",
    "This section can be used to reload and visualize previously saved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results (standalone cell)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Configuration - set these to match your saved results\n",
    "LOAD_DATASET = '50salads'  # or 'gtea', 'breakfast'\n",
    "LOAD_MODEL = 'asformer'    # or 'mstcn2', 'original'\n",
    "\n",
    "workspace_root = globals().get('workspace_root', '/home/dsi/eli-bogdanov/sktr_for_long_traces')\n",
    "\n",
    "results_dir = Path(workspace_root) / 'results' / LOAD_DATASET / 'variant_experiment' / LOAD_MODEL\n",
    "final_results_dir = results_dir / 'final_experiment'\n",
    "sweep_summary_path = final_results_dir / f\"{LOAD_DATASET}_{LOAD_MODEL}_sweep_summary.csv\"\n",
    "\n",
    "if sweep_summary_path.exists():\n",
    "    df = pd.read_csv(sweep_summary_path).sort_values('n_train_traces').reset_index(drop=True)\n",
    "    print(f\"Loaded: {sweep_summary_path}\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(f\"File not found: {sweep_summary_path}\")\n",
    "    print(f\"Run the experiment first or check the dataset/model configuration.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}